{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 0 ====================\n",
      "Input size: torch.Size([1, 4]) tensor([[-2.5935, -0.4623,  0.2474,  1.8475]])\n",
      "hidden size: torch.Size([1, 2]) tensor([[-0.6868, -0.3829]], grad_fn=<TanhBackward>)\n",
      "tensor([[-0.6868, -0.3829]], grad_fn=<TanhBackward>)\n",
      "==================== 1 ====================\n",
      "Input size: torch.Size([1, 4]) tensor([[-0.9006, -1.5897, -0.7521, -0.8903]])\n",
      "hidden size: torch.Size([1, 2]) tensor([[ 0.9607, -0.7583]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.9607, -0.7583]], grad_fn=<TanhBackward>)\n",
      "==================== 2 ====================\n",
      "Input size: torch.Size([1, 4]) tensor([[-0.1715, -0.1162, -0.2700,  0.0835]])\n",
      "hidden size: torch.Size([1, 2]) tensor([[-0.4698, -0.1793]], grad_fn=<TanhBackward>)\n",
      "tensor([[-0.4698, -0.1793]], grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 注 调用RNNCell这个需要循环，循环长度就是序列长度\n",
    "import torch\n",
    "\n",
    "batch_size = 1  # 批处理大小\n",
    "seq_len = 3     # 序列长度\n",
    "input_size = 4  # 输入维度\n",
    "hidden_size = 2 # 隐层维度\n",
    "\n",
    "cell = torch.nn.RNNCell(input_size=input_size, hidden_size=hidden_size)\n",
    "\n",
    "# (seq, batch, features)\n",
    "dataset = torch.randn(seq_len, batch_size, input_size)\n",
    "hidden = torch.zeros(batch_size, hidden_size)\n",
    "\n",
    "# 这个循环就是处理seq_len长度的数据\n",
    "for idx, data in enumerate(dataset):\n",
    "    print('=' * 20, idx, '=' * 20)\n",
    "    print('Input size:', data.shape, data)\n",
    "\n",
    "    hidden = cell(data, hidden)\n",
    "\n",
    "    print('hidden size:', hidden.shape, hidden)\n",
    "    print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output size: torch.Size([3, 1, 2])\n",
      "Output: tensor([[[-0.2217,  0.7189]],\n",
      "\n",
      "        [[-0.8985,  0.6864]],\n",
      "\n",
      "        [[-0.6135,  0.9297]]], grad_fn=<StackBackward>)\n",
      "Hidden size: torch.Size([1, 1, 2])\n",
      "Hidden: tensor([[[-0.6135,  0.9297]]], grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "# RNN\n",
    "\n",
    "import torch\n",
    "\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "input_size = 4\n",
    "hidden_size = 2\n",
    "num_layers = 1\n",
    "\n",
    "cell = torch.nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "\n",
    "# (seqLen, batchSize, inputSize)\n",
    "inputs = torch.randn(seq_len, batch_size, input_size)\n",
    "hidden = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "\n",
    "out, hidden = cell(inputs, hidden)\n",
    "\n",
    "print('Output size:', out.shape)        # (seq_len, batch_size, hidden_size)\n",
    "print('Output:', out)\n",
    "print('Hidden size:', hidden.shape)     # (num_layers, batch_size, hidden_size)\n",
    "print('Hidden:', hidden)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 4]) torch.Size([5, 1])\n",
      "Predicted string:leloo, Epoch [1/15] loss=6.5516\n",
      "Predicted string:lhloo, Epoch [2/15] loss=5.2902\n",
      "Predicted string:lhloo, Epoch [3/15] loss=4.6566\n",
      "Predicted string:lhloo, Epoch [4/15] loss=4.2846\n",
      "Predicted string:ohloo, Epoch [5/15] loss=4.0401\n",
      "Predicted string:ohlol, Epoch [6/15] loss=3.8526\n",
      "Predicted string:ohlol, Epoch [7/15] loss=3.6770\n",
      "Predicted string:ohlol, Epoch [8/15] loss=3.4852\n",
      "Predicted string:ohlol, Epoch [9/15] loss=3.2798\n",
      "Predicted string:ohlol, Epoch [10/15] loss=3.0964\n",
      "Predicted string:ohlol, Epoch [11/15] loss=2.9560\n",
      "Predicted string:ohlol, Epoch [12/15] loss=2.8546\n",
      "Predicted string:ohlol, Epoch [13/15] loss=2.7766\n",
      "Predicted string:ohlol, Epoch [14/15] loss=2.6903\n",
      "Predicted string:ohlol, Epoch [15/15] loss=2.5741\n"
     ]
    }
   ],
   "source": [
    "# 序列到序列的demo, 使用RNN Cell实现\n",
    "import torch\n",
    "input_size = 4\n",
    "hidden_size = 4\n",
    "batch_size = 1\n",
    "\n",
    "idx2char = ['e', 'h', 'l', 'o'] # 字典\n",
    "x_data = [1, 0, 2, 3, 3]    # hello中各个字符的下标\n",
    "y_data = [3, 1, 2, 3, 2]    # ohlol中各个字符的下标\n",
    "\n",
    "one_hot_lookup = [[1, 0, 0, 0],\n",
    "                  [0, 1, 0, 0],\n",
    "                  [0, 0, 1, 0],\n",
    "                  [0, 0, 0, 1]]\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data] # (seqLen, inputSize)\n",
    "\n",
    "inputs = torch.Tensor(x_one_hot).view(-1, batch_size, input_size)\n",
    "labels = torch.LongTensor(y_data).view(-1, 1)   # torch.Tensor默认是torch.FloatTensor是32位浮点类型数据，torch.LongTensor是64位整型\n",
    "print(inputs.shape, labels.shape)\n",
    "\n",
    "# 构建模型\n",
    "import torch.nn as nn\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnncell = nn.RNNCell(input_size=self.input_size, hidden_size=self.hidden_size)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        hidden = self.rnncell(inputs, hidden)   # (batch_size, hidden_size)\n",
    "        return hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(self.batch_size, self.hidden_size)\n",
    "\n",
    "net = Model(input_size, hidden_size, batch_size)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.1)\n",
    "\n",
    "# 训练数据\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    hidden = net.init_hidden()\n",
    "    print('Predicted string:', end='')\n",
    "    for input, label in zip(inputs, labels):\n",
    "        hidden = net(input, hidden)\n",
    "        # 注意交叉熵在计算loss的时候维度关系，这里的hidden是([1, 4]), label是 ([1])\n",
    "        loss += criterion(hidden, label)\n",
    "        _, idx = hidden.max(dim = 1)\n",
    "        print(idx2char[idx.item()], end='')\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(', Epoch [%d/15] loss=%.4f' % (epoch+1, loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 4]) torch.Size([5])\n",
      "Predicted:  lllll, Epoch [1/15] loss = 1.308\n",
      "Predicted:  lllll, Epoch [2/15] loss = 1.102\n",
      "Predicted:  ohool, Epoch [3/15] loss = 0.968\n",
      "Predicted:  ohooo, Epoch [4/15] loss = 0.908\n",
      "Predicted:  ohooo, Epoch [5/15] loss = 0.846\n",
      "Predicted:  ohool, Epoch [6/15] loss = 0.779\n",
      "Predicted:  ohlol, Epoch [7/15] loss = 0.724\n",
      "Predicted:  ohlol, Epoch [8/15] loss = 0.683\n",
      "Predicted:  ohlol, Epoch [9/15] loss = 0.648\n",
      "Predicted:  ohlol, Epoch [10/15] loss = 0.616\n",
      "Predicted:  ohlol, Epoch [11/15] loss = 0.588\n",
      "Predicted:  ohlol, Epoch [12/15] loss = 0.565\n",
      "Predicted:  ohlol, Epoch [13/15] loss = 0.547\n",
      "Predicted:  ohlol, Epoch [14/15] loss = 0.532\n",
      "Predicted:  ohlol, Epoch [15/15] loss = 0.516\n"
     ]
    }
   ],
   "source": [
    "## 4.2、RNN实现\n",
    "\n",
    "# **注意：**\n",
    "#\n",
    "# inputs和labels的维度\n",
    "# inputs维度是: (seqLen, batch_size, input_size)\n",
    "# labels维度是: (seqLen * batch_size)\n",
    "# 注意outputs维度，对应和labels做交叉熵的维度\n",
    "# outputs维度是: (seqLen, batch_size, hidden_size)\n",
    "# 为了能和labels做交叉熵，需要reshape一下: outputs.view(-1, hidden_size)\n",
    "# 数据准备\n",
    "\n",
    "import torch\n",
    "input_size = 4\n",
    "hidden_size = 4\n",
    "batch_size = 1\n",
    "seq_len = 5\n",
    "num_layers = 1\n",
    "\n",
    "idx2char = ['e', 'h', 'l', 'o']\n",
    "x_data = [1, 0, 2, 3, 3]    # hello中各个字符的下标\n",
    "y_data = [3, 1, 2, 3, 2]    # ohlol中各个字符的下标\n",
    "\n",
    "one_hot_lookup = [[1, 0, 0, 0],\n",
    "                  [0, 1, 0, 0],\n",
    "                  [0, 0, 1, 0],\n",
    "                  [0, 0, 0, 1]]\n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data] # (seqLen, inputSize)\n",
    "\n",
    "inputs = torch.Tensor(x_one_hot).view(seq_len, batch_size, input_size)\n",
    "labels = torch.LongTensor(y_data)\n",
    "print(inputs.shape, labels.shape)\n",
    "\n",
    "\n",
    "# 构建模型\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size, num_layers=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size=self.input_size, hidden_size=self.hidden_size, )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        hidden = torch.zeros(self.num_layers, self.batch_size, self.hidden_size)\n",
    "        out, _ = self.rnn(inputs, hidden)    # 注意维度是(seqLen, batch_size, hidden_size)\n",
    "        return out.view(-1, self.hidden_size) # 为了容易计算交叉熵这里调整维度为(seqLen * batch_size, hidden_size)\n",
    "\n",
    "net = Model(input_size, hidden_size, batch_size)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "# 模型训练\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "    # print(outputs.shape, labels.shape)\n",
    "    # 这里的outputs维度是([seqLen * batch_size, hidden]), labels维度是([seqLen])\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, idx = outputs.max(dim=1)\n",
    "    idx = idx.data.numpy()\n",
    "    print('Predicted: ', ''.join([idx2char[x] for x in idx]), end='')\n",
    "    print(', Epoch [%d/15] loss = %.3f' % (epoch + 1, loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  hooeh, Epoch [1/15] loss = 1.435\n",
      "Predicted:  ohool, Epoch [2/15] loss = 1.102\n",
      "Predicted:  lhool, Epoch [3/15] loss = 0.905\n",
      "Predicted:  lhlol, Epoch [4/15] loss = 0.757\n",
      "Predicted:  lhlol, Epoch [5/15] loss = 0.649\n",
      "Predicted:  lhlol, Epoch [6/15] loss = 0.575\n",
      "Predicted:  lhlol, Epoch [7/15] loss = 0.510\n",
      "Predicted:  ohlol, Epoch [8/15] loss = 0.443\n",
      "Predicted:  ohlol, Epoch [9/15] loss = 0.375\n",
      "Predicted:  ohlol, Epoch [10/15] loss = 0.311\n",
      "Predicted:  ohlol, Epoch [11/15] loss = 0.255\n",
      "Predicted:  ohlol, Epoch [12/15] loss = 0.207\n",
      "Predicted:  ohlol, Epoch [13/15] loss = 0.168\n",
      "Predicted:  ohlol, Epoch [14/15] loss = 0.137\n",
      "Predicted:  ohlol, Epoch [15/15] loss = 0.113\n"
     ]
    }
   ],
   "source": [
    "# 词向量方式\n",
    "\n",
    "# 数据准备\n",
    "idx2char = ['e', 'h', 'l', 'o']\n",
    "x_data = [[1, 0, 2, 2, 3]]  # (batch, seq_len)\n",
    "y_data = [3, 1, 2, 3, 2]    # (batch * seq_len)\n",
    "\n",
    "inputs = torch.LongTensor(x_data)   # Input should be LongTensor: (batchSize, seqLen)\n",
    "labels = torch.LongTensor(y_data)   # Target should be LongTensor: (batchSize * seqLen)\n",
    "\n",
    "\n",
    "# 构建模型\n",
    "import torch.nn as nn\n",
    "\n",
    "# parameters\n",
    "num_class = 4\n",
    "input_size = 4\n",
    "hidden_size = 8\n",
    "embedding_size = 10\n",
    "num_layers = 2\n",
    "batch_size = 1\n",
    "seq_len = 5\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.emb = torch.nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.RNN(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.zeros(num_layers, x.size(0), hidden_size)\n",
    "        x = self.emb(x)                 # (batch, seqLen, embeddingSize)\n",
    "        x, _ = self.rnn(x, hidden)      # 输出(𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆, 𝒔𝒆𝒒𝑳𝒆𝒏, hidden_size)\n",
    "        x = self.fc(x)                  # 输出(𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆, 𝒔𝒆𝒒𝑳𝒆𝒏, 𝒏𝒖𝒎𝑪𝒍𝒂𝒔𝒔)\n",
    "        return x.view(-1, num_class)    # reshape to use Cross Entropy: (𝒃𝒂𝒕𝒄𝒉𝑺𝒊𝒛𝒆×𝒔𝒆𝒒𝑳𝒆𝒏, 𝒏𝒖𝒎𝑪𝒍𝒂𝒔𝒔)\n",
    "\n",
    "net = Model()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.05)\n",
    "\n",
    "\n",
    "# 模型训练\n",
    "epochs = 15\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    _, idx = outputs.max(dim=1)\n",
    "    idx = idx.data.numpy()\n",
    "    print('Predicted: ', ''.join([idx2char[x] for x in idx]), end='')\n",
    "    print(', Epoch [%d/15] loss = %.3f' % (epoch + 1, loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# GRU 分类模型\n",
    "\n",
    "# 准备数据\n",
    "# 1、对于每个名字需要得到一个向量\n",
    "# 2、通过ASCII对于每个名字的每个字符都得到一个one-hot vector\n",
    "# 3、由于输入是矩阵所以需要padding\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import gzip\n",
    "import csv\n",
    "\n",
    "class NameDataset(Dataset):\n",
    "    def __init__(self, is_train_set):\n",
    "        filename = '../dataset/names_train.csv' if is_train_set else '../dataset/names_test.csv'\n",
    "        with open(filename, 'rt') as f:    # r表示只读，从文件头开始 t表示文本模式\n",
    "            reader = csv.reader(f)\n",
    "            rows = list(reader)\n",
    "        self.names = [row[0] for row in rows]\n",
    "        self.len = len(self.names)\n",
    "        self.countries = [row[1] for row in rows]\n",
    "\n",
    "        self.country_list = list(sorted(set(self.countries)))\n",
    "        self.country_dict = self.getCountryDict()\n",
    "        self.country_num = len(self.country_list)\n",
    "\n",
    "    def __getitem__(self, index):       # 根据索引拿到的是 名字，国家的索引\n",
    "        return self.names[index], self.country_dict[self.countries[index]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def getCountryDict(self):\n",
    "        country_dict = dict()\n",
    "        for idx, country_name in enumerate(self.country_list, 0):\n",
    "            country_dict[country_name] = idx\n",
    "        return country_dict\n",
    "\n",
    "    def idx2country(self, index):\n",
    "        return self.country_list[index]\n",
    "\n",
    "    def getCountriesNum(self):\n",
    "        return self.country_num\n",
    "\n",
    "HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 256\n",
    "N_LAYER = 2\n",
    "N_EPOCHS = 50\n",
    "N_CHARS = 128   # 这个是为了构造嵌入层\n",
    "\n",
    "trainSet = NameDataset(is_train_set=True)\n",
    "trainLoader = DataLoader(trainSet, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testSet = NameDataset(is_train_set=False)\n",
    "testLoader = DataLoader(testSet, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "N_COUNTRY = trainSet.getCountriesNum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "# GRU的维度\n",
    "# 输入维度：\n",
    "# 𝑖𝑛𝑝𝑢𝑡: (𝑠𝑒𝑞𝐿𝑒𝑛, 𝑏𝑎𝑡𝑐ℎ𝑆𝑖𝑧𝑒, ℎ𝑖𝑑𝑑𝑒𝑛𝑆𝑖𝑧𝑒)\n",
    "# hidden: (nLayers * nDirections, batchSize, hiddenSize)\n",
    "# 输出维度\n",
    "# output: (seqLen, batchSize, hiddenSize * nDirections)\n",
    "# hidden: (nLayers * nDirections, batchSize, hiddenSize)\n",
    "\n",
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_directions = 2 if bidirectional else 1   # 使用双向的GRU\n",
    "\n",
    "        # 嵌入层（𝑠𝑒𝑞𝐿𝑒𝑛, 𝑏𝑎𝑡𝑐ℎ𝑆𝑖𝑧𝑒） --> (𝑠𝑒𝑞𝐿𝑒𝑛, 𝑏𝑎𝑡𝑐ℎ𝑆𝑖𝑧𝑒, hidden_size)\n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, n_layers, bidirectional=bidirectional)\n",
    "        self.fc = torch.nn.Linear(hidden_size * self.n_directions, output_size)\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers * self.n_directions, batch_size, self.hidden_size)\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, input, seq_lengths):\n",
    "        # input shape : B x S -> S x B\n",
    "        input = input.t()\n",
    "        batch_size = input.size(1)\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        embedding = self.embedding(input)\n",
    "\n",
    "        # pack them up\n",
    "        gru_input = torch.nn.utils.rnn.pack_padded_sequence(embedding, seq_lengths)\n",
    "        output, hidden = self.gru(gru_input, hidden)\n",
    "        if self.n_directions == 2:\n",
    "            hidden_cat = torch.cat([hidden[-1], hidden[-2]], dim=1)\n",
    "        else:\n",
    "            hidden_cat = hidden[-1]\n",
    "        fc_output = self.fc(hidden_cat)\n",
    "        return fc_output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# 数据转换成Tensor\n",
    "def name2list(name):\n",
    "    arr = [ord(c) for c in name]\n",
    "    return arr, len(arr)\n",
    "\n",
    "def make_tensors(names, countries):\n",
    "    sequences_and_lengths = [name2list(name) for name in names]\n",
    "    name_sequences = [s1[0] for s1 in sequences_and_lengths]\n",
    "    seq_lengths = torch.LongTensor([s1[1] for s1 in sequences_and_lengths])\n",
    "    countries = countries.long()\n",
    "\n",
    "    # make tensor of name, BatchSize * seqLen\n",
    "    # 他这里补零的方式先将所有的0 Tensor给初始化出来，然后在每行前面填充每个名字\n",
    "    seq_tensor = torch.zeros(len(name_sequences), seq_lengths.max()).long()\n",
    "    # print(\"seq_lengths.max:\", seq_lengths.max())\n",
    "    for idx, (seq, seq_len) in enumerate(zip(name_sequences, seq_lengths), 0):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)\n",
    "\n",
    "    # sort by length to use pack_padded_sequence\n",
    "    # 将名字长度降序排列，并且返回降序之后的长度在原tensor中的小标perm_idx\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(dim=0, descending=True)\n",
    "    # 这个Tensor中的类似于列表中切片的方法神奇啊，直接返回下标对应的元素，相等于排序了\n",
    "    seq_tensor = seq_tensor[perm_idx]\n",
    "    countries = countries[perm_idx]\n",
    "\n",
    "    # 返回排序之后名字Tensor，排序之后的名字长度Tensor，排序之后的国家名字Tensor\n",
    "    return seq_tensor, seq_lengths, countries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Training for 50 epochs...\n",
      "[0m 3s] Epoch 1 [2560/13374] loss=0.008832729514688253\n",
      "[0m 5s] Epoch 1 [5120/13374] loss=0.007542191515676677\n",
      "[0m 7s] Epoch 1 [7680/13374] loss=0.006860072553778688\n",
      "[0m 10s] Epoch 1 [10240/13374] loss=0.006449167791288346\n",
      "[0m 13s] Epoch 1 [12800/13374] loss=0.00606359270401299\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 4486/6700 66.96%\n",
      "[0m 17s] Epoch 2 [2560/13374] loss=0.004324496164917946\n",
      "[0m 20s] Epoch 2 [5120/13374] loss=0.004129905730951578\n",
      "[0m 22s] Epoch 2 [7680/13374] loss=0.003995212229589621\n",
      "[0m 24s] Epoch 2 [10240/13374] loss=0.003893147746566683\n",
      "[0m 27s] Epoch 2 [12800/13374] loss=0.003799401265569031\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 4964/6700 74.09%\n",
      "[0m 31s] Epoch 3 [2560/13374] loss=0.00313174978364259\n",
      "[0m 33s] Epoch 3 [5120/13374] loss=0.0031205926090478895\n",
      "[0m 35s] Epoch 3 [7680/13374] loss=0.003105234323690335\n",
      "[0m 38s] Epoch 3 [10240/13374] loss=0.0030474235361907633\n",
      "[0m 40s] Epoch 3 [12800/13374] loss=0.003038507895544171\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5236/6700 78.15%\n",
      "[0m 44s] Epoch 4 [2560/13374] loss=0.0026674010790884496\n",
      "[0m 46s] Epoch 4 [5120/13374] loss=0.0025709983427077534\n",
      "[0m 49s] Epoch 4 [7680/13374] loss=0.0025776277063414453\n",
      "[0m 51s] Epoch 4 [10240/13374] loss=0.0025854019506368784\n",
      "[0m 53s] Epoch 4 [12800/13374] loss=0.002563250996172428\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5352/6700 79.88%\n",
      "[0m 57s] Epoch 5 [2560/13374] loss=0.0022169032832607626\n",
      "[1m 0s] Epoch 5 [5120/13374] loss=0.00224430788657628\n",
      "[1m 2s] Epoch 5 [7680/13374] loss=0.0022397467866539953\n",
      "[1m 5s] Epoch 5 [10240/13374] loss=0.0022307947219815105\n",
      "[1m 7s] Epoch 5 [12800/13374] loss=0.002224230628926307\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5454/6700 81.40%\n",
      "[1m 12s] Epoch 6 [2560/13374] loss=0.0018807423650287092\n",
      "[1m 14s] Epoch 6 [5120/13374] loss=0.0019480498624034226\n",
      "[1m 17s] Epoch 6 [7680/13374] loss=0.001970440646012624\n",
      "[1m 19s] Epoch 6 [10240/13374] loss=0.0019800230540568007\n",
      "[1m 21s] Epoch 6 [12800/13374] loss=0.001985084488987923\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5519/6700 82.37%\n",
      "[1m 25s] Epoch 7 [2560/13374] loss=0.0019465517019852997\n",
      "[1m 28s] Epoch 7 [5120/13374] loss=0.0018009873805567623\n",
      "[1m 30s] Epoch 7 [7680/13374] loss=0.00181429524673149\n",
      "[1m 32s] Epoch 7 [10240/13374] loss=0.0018090859113726765\n",
      "[1m 34s] Epoch 7 [12800/13374] loss=0.0018069191905669868\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5521/6700 82.40%\n",
      "[1m 39s] Epoch 8 [2560/13374] loss=0.0016136121354065835\n",
      "[1m 41s] Epoch 8 [5120/13374] loss=0.001609055628068745\n",
      "[1m 44s] Epoch 8 [7680/13374] loss=0.0016085412081641457\n",
      "[1m 46s] Epoch 8 [10240/13374] loss=0.001599978533340618\n",
      "[1m 49s] Epoch 8 [12800/13374] loss=0.0016165172844193876\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5581/6700 83.30%\n",
      "[1m 54s] Epoch 9 [2560/13374] loss=0.0013791684294119477\n",
      "[1m 56s] Epoch 9 [5120/13374] loss=0.0014420004852581769\n",
      "[1m 59s] Epoch 9 [7680/13374] loss=0.0014344724593684077\n",
      "[2m 1s] Epoch 9 [10240/13374] loss=0.001459980773506686\n",
      "[2m 3s] Epoch 9 [12800/13374] loss=0.001443005760665983\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5610/6700 83.73%\n",
      "[2m 8s] Epoch 10 [2560/13374] loss=0.0013037657714448868\n",
      "[2m 11s] Epoch 10 [5120/13374] loss=0.001262931409291923\n",
      "[2m 14s] Epoch 10 [7680/13374] loss=0.001295517738132427\n",
      "[2m 16s] Epoch 10 [10240/13374] loss=0.001316913074697368\n",
      "[2m 19s] Epoch 10 [12800/13374] loss=0.0013312010699883102\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5647/6700 84.28%\n",
      "[2m 23s] Epoch 11 [2560/13374] loss=0.0011685816221870481\n",
      "[2m 25s] Epoch 11 [5120/13374] loss=0.001143947907257825\n",
      "[2m 28s] Epoch 11 [7680/13374] loss=0.0011713298270478844\n",
      "[2m 30s] Epoch 11 [10240/13374] loss=0.0011576173899811692\n",
      "[2m 32s] Epoch 11 [12800/13374] loss=0.001170527352951467\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5645/6700 84.25%\n",
      "[2m 38s] Epoch 12 [2560/13374] loss=0.0010549832484684884\n",
      "[2m 40s] Epoch 12 [5120/13374] loss=0.0010450451169162988\n",
      "[2m 43s] Epoch 12 [7680/13374] loss=0.0010338154524409523\n",
      "[2m 45s] Epoch 12 [10240/13374] loss=0.0010379464598372578\n",
      "[2m 47s] Epoch 12 [12800/13374] loss=0.0010595982382074\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5657/6700 84.43%\n",
      "[2m 53s] Epoch 13 [2560/13374] loss=0.0009396007924806326\n",
      "[2m 55s] Epoch 13 [5120/13374] loss=0.000915722333593294\n",
      "[2m 58s] Epoch 13 [7680/13374] loss=0.000919005168058599\n",
      "[3m 0s] Epoch 13 [10240/13374] loss=0.0009299894503783434\n",
      "[3m 2s] Epoch 13 [12800/13374] loss=0.0009262237721122801\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5654/6700 84.39%\n",
      "[3m 6s] Epoch 14 [2560/13374] loss=0.0007284211926162243\n",
      "[3m 9s] Epoch 14 [5120/13374] loss=0.0007929059152957052\n",
      "[3m 11s] Epoch 14 [7680/13374] loss=0.0008027114206925034\n",
      "[3m 14s] Epoch 14 [10240/13374] loss=0.0008186244827811606\n",
      "[3m 17s] Epoch 14 [12800/13374] loss=0.0008296501007862389\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5632/6700 84.06%\n",
      "[3m 21s] Epoch 15 [2560/13374] loss=0.000713320163777098\n",
      "[3m 23s] Epoch 15 [5120/13374] loss=0.0007293961243703961\n",
      "[3m 26s] Epoch 15 [7680/13374] loss=0.0007303785804348687\n",
      "[3m 28s] Epoch 15 [10240/13374] loss=0.000731189051293768\n",
      "[3m 30s] Epoch 15 [12800/13374] loss=0.0007369769446086138\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5647/6700 84.28%\n",
      "[3m 35s] Epoch 16 [2560/13374] loss=0.0006305537215666845\n",
      "[3m 37s] Epoch 16 [5120/13374] loss=0.0006520874972920865\n",
      "[3m 39s] Epoch 16 [7680/13374] loss=0.0006496175396023318\n",
      "[3m 42s] Epoch 16 [10240/13374] loss=0.0006425591673178133\n",
      "[3m 44s] Epoch 16 [12800/13374] loss=0.0006419134425232187\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5651/6700 84.34%\n",
      "[3m 49s] Epoch 17 [2560/13374] loss=0.0005444812210043892\n",
      "[3m 51s] Epoch 17 [5120/13374] loss=0.0005458799641928636\n",
      "[3m 53s] Epoch 17 [7680/13374] loss=0.000560464322916232\n",
      "[3m 56s] Epoch 17 [10240/13374] loss=0.0005532984469027724\n",
      "[3m 58s] Epoch 17 [12800/13374] loss=0.0005661812575999647\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5617/6700 83.84%\n",
      "[4m 3s] Epoch 18 [2560/13374] loss=0.0004833873128518462\n",
      "[4m 6s] Epoch 18 [5120/13374] loss=0.0005027712555602193\n",
      "[4m 8s] Epoch 18 [7680/13374] loss=0.0005031370548143362\n",
      "[4m 10s] Epoch 18 [10240/13374] loss=0.0005015509035729337\n",
      "[4m 13s] Epoch 18 [12800/13374] loss=0.000526154597173445\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5618/6700 83.85%\n",
      "[4m 17s] Epoch 19 [2560/13374] loss=0.00044437880569603294\n",
      "[4m 19s] Epoch 19 [5120/13374] loss=0.00045534138480434193\n",
      "[4m 21s] Epoch 19 [7680/13374] loss=0.0004580839100526646\n",
      "[4m 24s] Epoch 19 [10240/13374] loss=0.0004615857389580924\n",
      "[4m 26s] Epoch 19 [12800/13374] loss=0.0004678059264551848\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5640/6700 84.18%\n",
      "[4m 31s] Epoch 20 [2560/13374] loss=0.0003822900034720078\n",
      "[4m 34s] Epoch 20 [5120/13374] loss=0.0003888897321303375\n",
      "[4m 38s] Epoch 20 [7680/13374] loss=0.0004131076003735264\n",
      "[4m 41s] Epoch 20 [10240/13374] loss=0.0004165894933976233\n",
      "[4m 44s] Epoch 20 [12800/13374] loss=0.00041971813596319407\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5623/6700 83.93%\n",
      "[4m 49s] Epoch 21 [2560/13374] loss=0.00040614639292471113\n",
      "[4m 51s] Epoch 21 [5120/13374] loss=0.0004097535420442\n",
      "[4m 54s] Epoch 21 [7680/13374] loss=0.00040351932548219336\n",
      "[4m 56s] Epoch 21 [10240/13374] loss=0.00041302523713966367\n",
      "[4m 59s] Epoch 21 [12800/13374] loss=0.00041154491453198716\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5605/6700 83.66%\n",
      "[5m 4s] Epoch 22 [2560/13374] loss=0.00030169215169735254\n",
      "[5m 6s] Epoch 22 [5120/13374] loss=0.0003302723605884239\n",
      "[5m 9s] Epoch 22 [7680/13374] loss=0.0003421287527695919\n",
      "[5m 11s] Epoch 22 [10240/13374] loss=0.00035637630498968064\n",
      "[5m 14s] Epoch 22 [12800/13374] loss=0.0003675986354937777\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5601/6700 83.60%\n",
      "[5m 18s] Epoch 23 [2560/13374] loss=0.000265572861826513\n",
      "[5m 21s] Epoch 23 [5120/13374] loss=0.00028590304718818517\n",
      "[5m 24s] Epoch 23 [7680/13374] loss=0.0003069104452151805\n",
      "[5m 26s] Epoch 23 [10240/13374] loss=0.0003104633735347306\n",
      "[5m 28s] Epoch 23 [12800/13374] loss=0.0003159022136242129\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5625/6700 83.96%\n",
      "[5m 32s] Epoch 24 [2560/13374] loss=0.0002462690914398991\n",
      "[5m 35s] Epoch 24 [5120/13374] loss=0.0002566524279245641\n",
      "[5m 37s] Epoch 24 [7680/13374] loss=0.00027193071922132126\n",
      "[5m 40s] Epoch 24 [10240/13374] loss=0.0002867190793040209\n",
      "[5m 43s] Epoch 24 [12800/13374] loss=0.0002964540375978686\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5591/6700 83.45%\n",
      "[5m 47s] Epoch 25 [2560/13374] loss=0.0002828516095178202\n",
      "[5m 50s] Epoch 25 [5120/13374] loss=0.0002809447323670611\n",
      "[5m 52s] Epoch 25 [7680/13374] loss=0.00029206621499421693\n",
      "[5m 55s] Epoch 25 [10240/13374] loss=0.000290798582136631\n",
      "[5m 58s] Epoch 25 [12800/13374] loss=0.0002972608484560624\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5627/6700 83.99%\n",
      "[6m 4s] Epoch 26 [2560/13374] loss=0.0002293573081260547\n",
      "[6m 7s] Epoch 26 [5120/13374] loss=0.00023149485350586473\n",
      "[6m 10s] Epoch 26 [7680/13374] loss=0.00025167378674571713\n",
      "[6m 13s] Epoch 26 [10240/13374] loss=0.00026506037247600036\n",
      "[6m 16s] Epoch 26 [12800/13374] loss=0.0002792250973288901\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5601/6700 83.60%\n",
      "[6m 21s] Epoch 27 [2560/13374] loss=0.0002095747178827878\n",
      "[6m 23s] Epoch 27 [5120/13374] loss=0.00023521416042058264\n",
      "[6m 25s] Epoch 27 [7680/13374] loss=0.0002502396996230042\n",
      "[6m 29s] Epoch 27 [10240/13374] loss=0.0002532485641495441\n",
      "[6m 33s] Epoch 27 [12800/13374] loss=0.0002637158993456978\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5596/6700 83.52%\n",
      "[6m 37s] Epoch 28 [2560/13374] loss=0.00022071147395763546\n",
      "[6m 39s] Epoch 28 [5120/13374] loss=0.0002457966824294999\n",
      "[6m 42s] Epoch 28 [7680/13374] loss=0.00025356093732019266\n",
      "[6m 44s] Epoch 28 [10240/13374] loss=0.00025826149503700436\n",
      "[6m 46s] Epoch 28 [12800/13374] loss=0.0002681384261813946\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5594/6700 83.49%\n",
      "[6m 51s] Epoch 29 [2560/13374] loss=0.00022220484825083986\n",
      "[6m 54s] Epoch 29 [5120/13374] loss=0.00021078071513329631\n",
      "[6m 56s] Epoch 29 [7680/13374] loss=0.0002215958195544469\n",
      "[6m 59s] Epoch 29 [10240/13374] loss=0.0002288868639880093\n",
      "[7m 2s] Epoch 29 [12800/13374] loss=0.00024237387086031959\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5616/6700 83.82%\n",
      "[7m 7s] Epoch 30 [2560/13374] loss=0.0001907492616737727\n",
      "[7m 9s] Epoch 30 [5120/13374] loss=0.0001980717650440056\n",
      "[7m 12s] Epoch 30 [7680/13374] loss=0.00021055457667292407\n",
      "[7m 15s] Epoch 30 [10240/13374] loss=0.00022710218581778463\n",
      "[7m 17s] Epoch 30 [12800/13374] loss=0.0002390780358109623\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5595/6700 83.51%\n",
      "[7m 22s] Epoch 31 [2560/13374] loss=0.000218482811760623\n",
      "[7m 24s] Epoch 31 [5120/13374] loss=0.00021981931822665502\n",
      "[7m 26s] Epoch 31 [7680/13374] loss=0.00024441310354935314\n",
      "[7m 28s] Epoch 31 [10240/13374] loss=0.00024124708616000135\n",
      "[7m 30s] Epoch 31 [12800/13374] loss=0.00025526273355353624\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5591/6700 83.45%\n",
      "[7m 34s] Epoch 32 [2560/13374] loss=0.00019285799644421786\n",
      "[7m 37s] Epoch 32 [5120/13374] loss=0.0002018814891926013\n",
      "[7m 39s] Epoch 32 [7680/13374] loss=0.0002039631634640197\n",
      "[7m 41s] Epoch 32 [10240/13374] loss=0.00021107818938617128\n",
      "[7m 44s] Epoch 32 [12800/13374] loss=0.00022607951861573384\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5623/6700 83.93%\n",
      "[7m 47s] Epoch 33 [2560/13374] loss=0.00020720468237414024\n",
      "[7m 49s] Epoch 33 [5120/13374] loss=0.00024110281410685274\n",
      "[7m 52s] Epoch 33 [7680/13374] loss=0.0002354764445044566\n",
      "[7m 54s] Epoch 33 [10240/13374] loss=0.00023769768176862272\n",
      "[7m 56s] Epoch 33 [12800/13374] loss=0.00023195240864879452\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5609/6700 83.72%\n",
      "[8m 0s] Epoch 34 [2560/13374] loss=0.000182860872882884\n",
      "[8m 2s] Epoch 34 [5120/13374] loss=0.00018948367760458497\n",
      "[8m 5s] Epoch 34 [7680/13374] loss=0.00020488666850724258\n",
      "[8m 7s] Epoch 34 [10240/13374] loss=0.0002139101545253652\n",
      "[8m 10s] Epoch 34 [12800/13374] loss=0.00021411845882539636\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5594/6700 83.49%\n",
      "[8m 14s] Epoch 35 [2560/13374] loss=0.00019215364009141921\n",
      "[8m 16s] Epoch 35 [5120/13374] loss=0.00019833688529615756\n",
      "[8m 18s] Epoch 35 [7680/13374] loss=0.0002053018812148366\n",
      "[8m 20s] Epoch 35 [10240/13374] loss=0.0002090040681650862\n",
      "[8m 22s] Epoch 35 [12800/13374] loss=0.00021109391062054783\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5608/6700 83.70%\n",
      "[8m 26s] Epoch 36 [2560/13374] loss=0.0001760134611686226\n",
      "[8m 28s] Epoch 36 [5120/13374] loss=0.00018604714168759528\n",
      "[8m 30s] Epoch 36 [7680/13374] loss=0.00019769853073133467\n",
      "[8m 32s] Epoch 36 [10240/13374] loss=0.0002024093499130686\n",
      "[8m 35s] Epoch 36 [12800/13374] loss=0.0002071880931907799\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5605/6700 83.66%\n",
      "[8m 38s] Epoch 37 [2560/13374] loss=0.00016333828389178962\n",
      "[8m 41s] Epoch 37 [5120/13374] loss=0.0001656270589592168\n",
      "[8m 43s] Epoch 37 [7680/13374] loss=0.00018269448313124788\n",
      "[8m 45s] Epoch 37 [10240/13374] loss=0.00018995878872374306\n",
      "[8m 47s] Epoch 37 [12800/13374] loss=0.00020790762835531495\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5615/6700 83.81%\n",
      "[8m 51s] Epoch 38 [2560/13374] loss=0.00015634297815267928\n",
      "[8m 53s] Epoch 38 [5120/13374] loss=0.00016507337350049056\n",
      "[8m 55s] Epoch 38 [7680/13374] loss=0.00017499165381498945\n",
      "[8m 57s] Epoch 38 [10240/13374] loss=0.00019242498256062391\n",
      "[8m 59s] Epoch 38 [12800/13374] loss=0.0002051904004474636\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5590/6700 83.43%\n",
      "[9m 4s] Epoch 39 [2560/13374] loss=0.00017308512396994047\n",
      "[9m 6s] Epoch 39 [5120/13374] loss=0.00018118142907042056\n",
      "[9m 8s] Epoch 39 [7680/13374] loss=0.00018178014579461887\n",
      "[9m 10s] Epoch 39 [10240/13374] loss=0.00020045189812663012\n",
      "[9m 12s] Epoch 39 [12800/13374] loss=0.00020768133850651793\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5594/6700 83.49%\n",
      "[9m 16s] Epoch 40 [2560/13374] loss=0.00018551565808593294\n",
      "[9m 18s] Epoch 40 [5120/13374] loss=0.00019762033880397212\n",
      "[9m 21s] Epoch 40 [7680/13374] loss=0.00018728879137294523\n",
      "[9m 23s] Epoch 40 [10240/13374] loss=0.00018723012681221007\n",
      "[9m 26s] Epoch 40 [12800/13374] loss=0.00019947904678701889\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5613/6700 83.78%\n",
      "[9m 30s] Epoch 41 [2560/13374] loss=0.0001697141804470448\n",
      "[9m 32s] Epoch 41 [5120/13374] loss=0.00017081074856832855\n",
      "[9m 35s] Epoch 41 [7680/13374] loss=0.00019159928742737976\n",
      "[9m 37s] Epoch 41 [10240/13374] loss=0.00019638333624243386\n",
      "[9m 39s] Epoch 41 [12800/13374] loss=0.0002045385182282189\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5599/6700 83.57%\n",
      "[9m 43s] Epoch 42 [2560/13374] loss=0.00015551426840829664\n",
      "[9m 45s] Epoch 42 [5120/13374] loss=0.00017107061030401382\n",
      "[9m 47s] Epoch 42 [7680/13374] loss=0.00019421132092247717\n",
      "[9m 49s] Epoch 42 [10240/13374] loss=0.00019319833427289268\n",
      "[9m 51s] Epoch 42 [12800/13374] loss=0.0001979075487179216\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5615/6700 83.81%\n",
      "[9m 55s] Epoch 43 [2560/13374] loss=0.0001798940349544864\n",
      "[9m 57s] Epoch 43 [5120/13374] loss=0.00018349545171076899\n",
      "[9m 59s] Epoch 43 [7680/13374] loss=0.00018511658563511447\n",
      "[10m 1s] Epoch 43 [10240/13374] loss=0.00019338934143888764\n",
      "[10m 4s] Epoch 43 [12800/13374] loss=0.00019675008676131256\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5585/6700 83.36%\n",
      "[10m 9s] Epoch 44 [2560/13374] loss=0.00015522049798164517\n",
      "[10m 11s] Epoch 44 [5120/13374] loss=0.00016706229907867965\n",
      "[10m 14s] Epoch 44 [7680/13374] loss=0.00017191306469612754\n",
      "[10m 16s] Epoch 44 [10240/13374] loss=0.00018982489946210989\n",
      "[10m 18s] Epoch 44 [12800/13374] loss=0.00019662173057440668\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5598/6700 83.55%\n",
      "[10m 21s] Epoch 45 [2560/13374] loss=0.00015363078637165017\n",
      "[10m 23s] Epoch 45 [5120/13374] loss=0.00015133832494029775\n",
      "[10m 26s] Epoch 45 [7680/13374] loss=0.0001715118793072179\n",
      "[10m 28s] Epoch 45 [10240/13374] loss=0.00018233247701573418\n",
      "[10m 30s] Epoch 45 [12800/13374] loss=0.00018915576583822258\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5607/6700 83.69%\n",
      "[10m 35s] Epoch 46 [2560/13374] loss=0.00014134406083030627\n",
      "[10m 37s] Epoch 46 [5120/13374] loss=0.00016937685868469998\n",
      "[10m 39s] Epoch 46 [7680/13374] loss=0.00016374278638977557\n",
      "[10m 41s] Epoch 46 [10240/13374] loss=0.00017390189441357506\n",
      "[10m 43s] Epoch 46 [12800/13374] loss=0.0001787801286263857\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5606/6700 83.67%\n",
      "[10m 46s] Epoch 47 [2560/13374] loss=0.00016765866748755798\n",
      "[10m 48s] Epoch 47 [5120/13374] loss=0.0001619914208276896\n",
      "[10m 50s] Epoch 47 [7680/13374] loss=0.00016273025539703667\n",
      "[10m 52s] Epoch 47 [10240/13374] loss=0.00017656539021118078\n",
      "[10m 55s] Epoch 47 [12800/13374] loss=0.00018257044263009448\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5583/6700 83.33%\n",
      "[10m 58s] Epoch 48 [2560/13374] loss=0.00014839903742540627\n",
      "[11m 0s] Epoch 48 [5120/13374] loss=0.00015991782602213788\n",
      "[11m 3s] Epoch 48 [7680/13374] loss=0.00016206034682303045\n",
      "[11m 5s] Epoch 48 [10240/13374] loss=0.00017316959474555915\n",
      "[11m 7s] Epoch 48 [12800/13374] loss=0.00017833391277235933\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5605/6700 83.66%\n",
      "[11m 11s] Epoch 49 [2560/13374] loss=0.00011969910301559139\n",
      "[11m 13s] Epoch 49 [5120/13374] loss=0.0001508362487584236\n",
      "[11m 15s] Epoch 49 [7680/13374] loss=0.00015923258433758747\n",
      "[11m 18s] Epoch 49 [10240/13374] loss=0.0001695543031928537\n",
      "[11m 20s] Epoch 49 [12800/13374] loss=0.00017914191987074445\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5615/6700 83.81%\n",
      "[11m 24s] Epoch 50 [2560/13374] loss=0.00013452148486976513\n",
      "[11m 26s] Epoch 50 [5120/13374] loss=0.0001499563062679954\n",
      "[11m 28s] Epoch 50 [7680/13374] loss=0.00016518103123720114\n",
      "[11m 30s] Epoch 50 [10240/13374] loss=0.0001680552510151756\n",
      "[11m 32s] Epoch 50 [12800/13374] loss=0.00017840594475273974\n",
      "evaluating trained model ... \n",
      "Test set: Accuracy 5618/6700 83.85%\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEHCAYAAAC5u6FsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAvlElEQVR4nO3de3xU1b338c+PXCGBhGu4hJuCclNRELwbtSreiq0ei6et2lqpbfVpbZ9W7emx1lN7PM9zPPa09ekpVrzWUi9VqWIRlVRrAQFBbnK/JtxJICQhJJP5PX/MDg4hJBPIMEnm+3695pXZa9beWT8yzG/2Wmuvbe6OiIhIrDokugEiItK2KHGIiEizKHGIiEizKHGIiEizKHGIiEizKHGIiEizpMbz4GY2AfhvIAX4vbs/Uu/1AcAzQG5Q5z53n2Fmg4BPgVVB1bnufmewzxjgaaAjMAP4rjcxp7hHjx4+aNCgRttaUVFBVlZWM6JrHxR3clHcyeV44164cOFud+95xAvuHpcHkUSwDjgJSAc+AUbUqzMF+FbwfASwMXg+CFh2lON+BJwDGPAWcFVTbRkzZow3Zfbs2U3WaY8Ud3JR3MnleOMGFngDn6nx7KoaB6x19/XuXg1MAybWq+NAl+B5DrC1sQOaWR+gi7vPDYJ6Fri+RVstIiKNimfi6AdsidouCsqiPQh8xcyKiHQ73R312mAzW2RmfzOzC6OOWdTEMUVEJI7iOsYRg5uBp939UTM7F3jOzEYB24AB7r4nGNN4zcxGNufAZjYZmAyQl5dHYWFho/XLy8ubrNMeKe7koriTS7zijmfiKAb6R23nB2XRbgcmALj7HDPLBHq4+07gYFC+0MzWAacE++c3cUyC/aYQGUNh7NixXlBQ0GhjCwsLaapOe6S4k4viTi7xijueXVXzgaFmNtjM0oFJwPR6dTYDlwGY2XAgE9hlZj3NLCUoPwkYCqx3921AmZmdY2YG3AK8HscYRESknridcbh7yMzuAmYSmWE11d2Xm9lDREbqpwM/AJ4ws3uIDJTf5u5uZhcBD5lZDRAG7nT3kuDQ3+az6bhvBQ8RETlB4jrG4e4ziAx6R5c9EPV8BXB+A/u9ArxylGMuAEa1bEtFRCRWiR4clzYiVBtm7voSFm8ppXNmGt2y0umelU637HS6ZaXTrVM6qSlaiEAkGShxyFFVh8J8uG43by3dxqwVOyitrDlq3dQOxj+PH8APLj+VnE5pMf+OcNjZXXGQ7fuq2Laviu37quialc51p/chMowlIq2NEkeSqqkN829vrOCDFQeYsmYu2RmpZGem0jn4uXVvFe98uoP9VSE6Z6Ry2fBeTBjVhwuG9uBAdS0lFdVRj4MsKy7j+bmbeHPJNu69ahg3npVPhw5HfvCHw84/1u3hhY828cmWfewoqyIUPnLFmAUbS3jwupENHkNEEkuJIwnV1Ib57rRFzFi6nWHdOlAdCrO5opL9VSHKD0Ye2RmpTBjZm6tO6835Q3qQkZpyaP/sjFR6ds444ri3nDeQB15fzo9eXsIfP9rMv00cxah+OQCUVlTz0sItvDBvMxv3VNItK52LT+lJ39xMenfJpHdOR/rkZJLXJZMnPljPlPfXs7eyhv/8pzNIT21+F1hZVQ3PzdlETsc0bjgrn47pKU3vJCIxUeJIMqHaMN+btpgZS7fzk2uGM6R2MwUF5x1Wx4M1I5vbVTSybw4vffNc/ryomEfe+pTrfvN3Jp09gKqaWt5cuo3qUJizB3XlnstPYcKo3oclo2g/vno4XTul8x9/XUlZVQ2//fKYmD/43Z3XFhfz8Jsr2V1+EID/mrWa284bxFfPGUjXrPRmxSQiR1LiSCKh2jDf+9Ni3ly6jX+5ejjfuPAkCgs3H1HveMYWOnQwbhyTz+Uj8nhs1mqenbORTumpTDq7P18eP5BTe3eO6TjfKjiZ3E5p/MurS/nKk/OYeuvZTY6dfLqtjJ++vpyPNpZwRn4OT946lqqaWn73/nr+a9Zqflu4ji+d3Z/bLxh8zPHVV1VTS2Za7Gcza3fup3hvFRefcuSCoyJthRJHkgjVhrnnxU94Y8k27r9qGHdcdFJcf19OxzQe/PxIvnPJELIyUuiU3vy32s3jBpDTMY3vTVvMl6bM4dmvj6NXl8wj6u07UMNjs1bz3NxNdMlM5ZEvnsZNY/sfGh8Zf1J3Vm3fz5T31/P83E08N3cT4/I6kD9iP0N6xZbI6qupDTPl/fX89ztruO6Mvjxyw2mkNTGrbNHmUm6Z+hH7q0J84cx+/GziSLpkxj6R4ETbuLuCx95ZzdBe2dwwJp8+OR0T3SRpJZQ4kkCoNsz3X/yEv3yylfuuGsY3Lz75hP3uhsZCmuPq0/rQJTONyc8t4Jpf/52+OZkcDIWprg1THYo8yqpqOBgK88/jBvDDK08lt9OR3VGn9u7MozedwQ+uOIWpf9/As3M2cPlj73P1qD7cdekQhvfp0sBvb9iy4n386OUlrNhWxhn9c3nl4yL2HajmN/981lHPPhZsLOG2p+bTPTudL48fyBMfrOejDSX8ctJozh7U7Zj/feoL1Yb588fFpKYYE0f3I+UYJxcs3FTKHc8uoLI6xOuLwzw6azUXDu3JTWPz+dzwvGadZR2LzXsq+aRoL+ee3J0e2cf3HgIIN37LnsMcqK7l3ZU7GDe4G706H/lFpTU5GKpl4cZSVmwrY09FNSXl1ZGfFQcpqahmx75KZp5WyYDunVr09ypxtHMHQ7X88KUlTP9kK/dOGMadJzBptJQLhvbghTvO4b/fWU2tQ3pKBzJSO5Ce2oH0lA50TE/hhrPyOS0/p8lj9c3tyE+uHcHp6TtYGe7Ds3M28ebSbVwxIo//ddnQQ4P5DamqqeXX763hf/62nq6d0vmfr5zFhFF9eG7ORh6Yvpxbpn7E728de8RZxNz1e/j60/Pp3SWTF+44h945mVw+Io97/rSYL/1uDt8qOJnvfe6UJs9YmjJv/R5+On05K7fvB2DK++t54LoRnHdyj2Yd562l2/jenxbTJyeTV751HilmvLxwCy8vLOKuFxaR0zGNiaP7MunsAYzoG1vC3V9Vw/pdFeR2ilwDlJ2ReliX6L4DNcxZt5sP1kQem0sqAeiWlc4vvjCKCaP6NCuGOnPW7eHRt1exaHMllxct5Kaz87loaM8Grznatu8Az83ZxAsfbWZvZQ15XTL43VfHMrp/7jH9boiMuZUfDJHaIfJ+PdZEHn281TvK+WDNLj5Ys5t5G/ZQVRMGIC3F6Nopcl1V9+x0Tuuay8lZ1WSktfz1VebNyMRt1dixY33BggWN1mmPi6Bt23eAO5//mE+27OVHE07l2wVDjqjTHuOORV3ceyureerDjTz14QbKqkKMG9SNk3tl0btLR3rnZBya7VVSUc2/vLqUdbsquOGsfP712uGHndm8vriYH7z4Caf27swzXx936Fvyh2t3c/sz88nv2okX7hh/2DfY8oMhfjZ9OS8tLOL0/Bwe+9JoTu6Z3exYdpZV8YsZn/La4q30y+3Iv147nFDY+fcZKynee4ArRuTx46uHM6hHVqN/b3fn9x9s4BdvfcpZA7ryxC1j6RY1maA27Pxj3W5eWlDEX5dvpzoU5swBuXxl/ECuOb3PEWch4bAzb0MJLy3Ywoxl2w59wEEk+XfNSqNbVgYpHWDF1jLCDlnpKZx7cncuGNKDIb0688hfP2VZcRnXj+7Lzz4/KuZrhD7eXMqjb6/iw7V7yOuSwaldalle2oE9FdX06pzBF8/K55/G5nNyz2w+3lzK1L9v4K1l23F3rhgRmU34n2+vYkfZQR754ml88az8pn9p8G+4YXcF8zaU8NGGEuat38PWfVWHXk/pYKSnRJJIdkYqXzt/ELedN6jJi2f3HajhN++t4fXFW9m5PzLp4+SeWVw4tCcXDu3BmIFdyemYdsT45PH+/zazhe4+9ohyJY6I9vYBOmfdHu564WOqamp59KYzjvqNrb3FHav6cddN3/3rsu1s21d1aEZWtH65HXn4C6MoOLVXw8dctZM7n19In5yOPPv1cazfXcHkZxcwuEcWz39j/FG7XN5auo37X13K3soaemRnMLB7JwZ260T/bp0Y2L0TA7p1IrdTGtkZaWRnptIpLYUOHYya2jDP/GMjv3xnDdWhMN+8+CS+XTDk0Ay0qppanvz7Bh6fvZaa2jBfO38wZ6Rt55rLLzmiDbVh52d/Wc6zczZxzWl9ePSmMxrtjtpXWcPLHxfxh3mbWL+rgpyOadw4Jp8vjx9ARloKryws4qWFW9hScoDOGalce0ZfLj6lJ+UHQ5RUHDzUrVJaWc2BmlrOGtCVC4f25MwBuYededXUhnl89lp+895aumen88gNp3PJUf79IdKN+F+zVvPeyp10z0rn25cM4cvjBzD3ww8474KLmL1qJy8t2MLsVbuoDTv9cjtSvPcAnTMjEzhuOXcQ/btFunVKKqr5zh8+Zs76Pdxx4WDunTCswQ/4/VU1vL18B++t2slHG0rYFXyw98jOYPzgbofOYqtDYapraw91sa7dVc6Ha/cwok8X/v2Lp3FGA2c24bAfmqW4p6KaCSN7U3BqTy4Y2pN+uU2POSlxHIdkShzuzpN/38C/v7WSQd078buvjml0ALi9xN1cTcVdHQqzo6yK7WWRq9krDoa49oy+ZGc03ru7cFMJX3tqPhlpKeyrrGFIr2ye/8b4w765N2RHWRUvLyxi054KNu2pZHNJJdvLqmjov6cZZKenYgZlVSEuObUnP71uJIN6NHxv6Z1lVfzn26t4aWEROPTonEGfnMj1M31yMsnLyWT+hhJmr9rFNy86iXsnDIv5wkt3Z876Pfxh3mZmLttOKOyYgTucd3J3bhrbnytH9j7u62iWFe/j+y8uZvWOciad3Z+rT+vD9n2Rv09kxYEDbNtXxcrt+8npmMY3Lz6JW88dRFbw96r/9965v4rXFhXzj3V7uHRYL244K/9Q3Wg1tWF+/sYKnpmziQuH9uA3N59FTqc0DoZq+duqXby+eCvvfLqDg6Ewvbtkcs5J3Rh/UnfGDe7GST2yGp2h6O78ddl2fjp9ObvKD3LruYP4wRWn0Dno6ly+dR8PvL6chZtKOXNA7mHXRcVKieM4JEviqKwO8aOXl/DGkm1cOTKP//ynMw69CY+mPcR9LOIZ98rtZdzy5Ef0zsnk2a+Pa3CwPhZVNbUUlR6gqLSSsqoQ5VUhyg/WUF4VYv/BEAeqa7lseB6fG94rpinUy4r38cSMeWR27X0oIW7bd4CyqhApHYwHPz+Sr54z8JjaCpEP41cWFlMbDjNxdL9D39xbysFQLY/NWsOU99cRvdhA96x0eudEkuAZ+bncct4gcjoe/r4/3r/3tI8286+vL6NfbkfOOak7M5Zuo6wqRPesdK49vQ+fH92PswbkHtNU9rKqGh6duYpn526iV+cMfnz1cBZuKuX5uZvo2im90ZUYmhKvxKHB8XaipKKam6fMZc3O/cEg+Ela6ylBhvXuwvs/uoTUDnZcCz9mpqUwpFc2Q3o1f9yjIaP65fCFoekUFJx+WHlldYiaWj/iw7a5enXO5FsF8Zt8kZGawn1XDePGMf0oqaihT04mvbpkHPVC0pY0adwAhvTK5s7nP2b6J1u5cmRvJo7uy/lDehz3pIYumWn8bOIovnBWPvf/eSnfnbaYDgZfPWcg32/m2m8nihJHO/Hwm5+yblc5T39tHBfp4rKEi/d01ZZ0LNfYJNKxXntzvMYO6sac+y+lNuxx+fuO7p/LX+46n9cWb2VEny4xz1hLhLb1jpEGzVu/h1c+LuJbBScraYjEUVpKB+L5nSA1pQM3joltBlci6QYKbVx1KMxPXov0vf6vS4cmujkikgR0xtHGPfn3DazZWc7vbxmrFWBF5ITQGUcbVlRaya/eXcPlI/L43Ii8RDdHRJJEXBOHmU0ws1VmttbM7mvg9QFmNtvMFpnZEjO7Oii/3MwWmtnS4OelUfsUBsdcHDyOfjVQO/ezv6wA4KfXjUhwS0QkmcStq8rMUoDHgcuBImC+mU139xVR1X4CvOjuvzWzEcAMYBCwG7jO3bea2ShgJtAvar8vu3vjF2a0c++s2MGsFTu4d8Iw8ru27Hx5EZHGxPOMYxyw1t3Xu3s1MA2YWK+OA3VzznKArQDuvsjdtwbly4GOZnb8S2S2E5XVIX46fTlDe2W36L0lRERiEc/E0Q/YErVdxOFnDQAPAl8xsyIiZxt3N3CcG4CP3T168aCngm6qf7UkvMrt1++tpXjvAX5+/ahjuq2qiMjxiNuSI2Z2IzDB3b8RbH8VGO/ud0XV+X7QhkfN7FzgSWCUu4eD10cC04Er3H1dUNbP3YvNrDPwCvC8uz/bwO+fDEwGyMvLGzNt2rRG21teXk52dstcodsSasLO+r1hDoScqhBU1ToHQlBR47y1oYZz+qRyx+nHfxLW2uI+URR3clHcx+aSSy454UuOFAP9o7bzg7JotwMTANx9jpllAj2AnWaWD7wK3FKXNIJ6xcHP/Wb2ApEusSMSh7tPAaZAZK2qptZraU1rNrk7t0z9iA/W7D7iNTMY2iubX339HLq3wA1uWlPcJ5LiTi6Ku2XFM3HMB4aa2WAiCWMS8M/16mwGLgOeNrPhQCawy8xygTeB+9z9w7rKZpYK5Lr7bjNLA64F3oljDAkxa8UOPlizm7svHcJlw/PIzkilc2Yq2RmpdAyW1BYRSZS4JQ53D5nZXURmRKUAU919uZk9BCxw9+nAD4AnzOweIgPlt7m7B/sNAR4wsweCQ14BVAAzg6SRQiRpPBGvGBKhOhTmFzM+ZUivbL572dDjWiRPRCQe4nrluLvPIDLoHV32QNTzFcD5Dez3c+DnRznsmJZsY2vz7JyNbNxTyVNfO1tJQ0RaJX0ytSKlFdX86t01XHRKz0bvciYikkhKHK3IL99ZTUV1LT+5ZniimyIiclRKHK3E2p3lPD9vMzeP688peYm534CISCyUOFqJX8z4lE5pKdzzuVMS3RQRkUYpcbQCH6zZxXsrd3LXpUNa5NoMEZF4UuJIsFBtmJ+/8SkDunXitvMHJbo5IiJNUuJIsD8t2MKqHfu5/6phZKTqRkwi0vopcSRQbdj59btrOXtQVyaM6p3o5oiIxESJI4HmrNvD9rIqvnb+YJJwkV8RaaOUOBLoz4uK6JyZyqXDdLGfiLQdShwJUlkdYuay7VxzWh8y0zS2ISJthxJHgsxasYOK6lquP7P+va1ERFo3JY4EeXVRMf1yOzJuULdEN0VEpFmUOBJg1/6DfLBmNxNH99W9NUSkzVHiSIA3lmylNux8Qd1UItIGKXEkwKuLihnZtwtDtZihiLRBShwn2Nqd5Swp2qezDRFps5Q4TrDXFxfTweDzZ/RNdFNERI6JEscJFA47ry4q5oKhPenVJTPRzREROSZKHCfQws2lFJUe4Atn6mxDRNquuCYOM5tgZqvMbK2Z3dfA6wPMbLaZLTKzJWZ2ddRr9wf7rTKzK2M9Zmv26qJiOqalcMUILWgoIm1X3BKHmaUAjwNXASOAm81sRL1qPwFedPczgUnA/wv2HRFsjwQmAP/PzFJiPGardDBUy5tLtjFhVG+yMlIT3RwRkWMWzzOOccBad1/v7tXANGBivToOdAme5wBbg+cTgWnuftDdNwBrg+PFcsxWafbKXew7UKMlRkSkzYvnV99+wJao7SJgfL06DwJvm9ndQBbwuah959bbt+4Tt6ljAmBmk4HJAHl5eRQWFjba2PLy8ibrHI8nFlWRk2GEipZRuLX1XC0e77hbK8WdXBR3y0p0n8nNwNPu/qiZnQs8Z2ajWuLA7j4FmAIwduxYLygoaLR+YWEhTdU5VvsO1LB01jt85ZxBXHZp6+pZi2fcrZniTi6Ku2XFM3EUA/2jtvODsmi3ExnDwN3nmFkm0KOJfZs6Zqsze+VOqmvDXHtGn0Q3RUTkuMVzjGM+MNTMBptZOpHB7un16mwGLgMws+FAJrArqDfJzDLMbDAwFPgoxmO2OjOXb6dX5wxG5+cmuikiIsctbmcc7h4ys7uAmUAKMNXdl5vZQ8ACd58O/AB4wszuITJQfpu7O7DczF4EVgAh4DvuXgvQ0DHjFUNLqKqppXDVLm4ck6+VcEWkXYjrGIe7zwBm1Ct7IOr5CuD8o+z7MPBwLMdszd5fvYsDNbVcOVLXbohI+6Arx+Ns5vId5HRMY/xJumGTiLQPShxxFKoN8+7KHVw2rBdpKfqnFpH2QZ9mcfTRhhL2VtZwhbqpRKQdUeKIo78u305mWgcuPqVnopsiItJilDjiJBx23l6+g4tP6UnH9JREN0dEpMUoccTJkuJ9bC+r0mwqEWl3lDjiZOby7aR2MC4blpfopoiItCgljjhwd2Yu2845J3Unp1NaopsjItKilDjiYO3OctbvruDKUeqmEpH2R4kjDmYu3w7AFSPUTSUi7Y8SRxzMXL6DMwfkktclM9FNERFpcUocLayotJKlxfs0m0pE2i0ljhb29vIdAEocItJuKXG0sJnLt3NqXmcG98hKdFNEROJCiaMF7Sk/yPyNJVw5UoPiItJ+KXG0oPkbSwk7FAzrleimiIjEjRJHC9pcUgHAkF7ZCW6JiEj8KHG0oE17KsntlEaXTF0tLiLtlxJHC9pcUsnAbp0S3QwRkbiKa+IwswlmtsrM1prZfQ28/piZLQ4eq81sb1B+SVT5YjOrMrPrg9eeNrMNUa+NjmcMzbG5pJIB3TWbSkTat9R4HdjMUoDHgcuBImC+mU139xV1ddz9nqj6dwNnBuWzgdFBeTdgLfB21OF/6O4vx6vtxyJUG6a49ADXnt4n0U0REYmreJ5xjAPWuvt6d68GpgETG6l/M/DHBspvBN5y98o4tLHFbN1bRSjsDOymMw4Rad+aTBxmdp2ZHUuC6QdsidouCsoa+h0DgcHAew28PIkjE8rDZrYk6OrKOIa2tbjNJZG81l9jHCLSzpm7N17B7HngXOAVYKq7r4zpwGY3AhPc/RvB9leB8e5+VwN17wXy3f3ueuV9gCVAX3eviSrbDqQDU4B17v5QA8ecDEwGyMvLGzNt2rRG21teXk529rFPo529uYZnVlTz6MUd6d6x7cw5ON642yrFnVwU97G55JJLFrr72CNecPcmH0AX4JvAXGAOkQ/kzk3scy4wM2r7fuD+o9RdBJzXQPl3gSmN/I4C4I2m2j9mzBhvyuzZs5us05hfvLnCh/54htfWho/rOCfa8cbdVinu5KK4jw2wwBv4TI3pq7G7lwEvExmn6AN8Afg4GNA+mvnAUDMbbGbpRLqcptevZGbDgK5BQqrviHGP4IwDMzPgemBZLDHE2+aSSvK7daRDB0t0U0RE4qrJWVVm9nnga8AQ4FlgnLvvNLNOwArg1w3t5+4hM7sLmAmkEOnmWm5mDxHJYnVJZBIwLchu0b93ENAf+Fu9Q//BzHoCBiwG7owl0HjbtEfXcIhIcohlOu4NwGPu/n50obtXmtntje3o7jOAGfXKHqi3/eBR9t1IA4Pp7n5pDG0+odydLSWVjBvcLdFNERGJu1gSx4PAtroNM+sI5Ln7Rnd/N14Na0tKK2vYfzCkGVUikhRiGeN4CQhHbdcGZRLYtCeyuKG6qkQkGcSSOFI9cgEfAMHz9Pg1qe2pu4ZjQHclDhFp/2JJHLuCAXIAzGwisDt+TWp7Nu8JEofOOEQkCcQyxnEnkZlMvyEyk2kLcEtcW9XGbCqpJK9LBplpKYluiohI3DWZONx9HXCOmWUH2+Vxb1Ubs7mkUmcbIpI0Ylod18yuAUYCmZHr7sAbWOYjWW3eU8n5Q3okuhkiIidELIsc/g/wJeBuIl1V/wQMjHO72oyqmlq2l1UxUAPjIpIkYhkcP8/dbwFK3f1nRNagOiW+zWo7iko1MC4iySWWxFEV/Kw0s75ADZH1qoTIUiOgqbgikjxiGeP4i5nlAv8X+Bhw4Il4NqotOXQNh844RCRJNJo4ghs4vevue4FXzOwNINPd952IxrUFm/ZUkpWeQvcsXRMpIsmh0a4qdw8TuW943fZBJY3DbS6pZED3LOpmm4mItHexjHG8a2Y3mD4ZGxS5hqNjopshInLCxJI4vklkUcODZlZmZvvNrCzO7WoTwmFnc0klA7tnJbopIiInTCxXjnc+EQ1pi3bsr6I6FNbAuIgklVjuAHhRQ+X1b+yUjLS4oYgko1im4/4w6nkmMA5YCLS6O/GdaJuCqbi6alxEkkksXVXXRW+bWX/gl/FqUFuypaSSlA5G31wNjotI8ohlcLy+ImB4LBXNbIKZrTKztWZ2XwOvP2Zmi4PHajPbG/VabdRr06PKB5vZvOCYfzKzhF1AsWlPJX1zM0lLOZZ/RhGRtimWMY5fE7laHCKJZjSRK8ib2i+FyDUglxNJNvPNbLq7r6ir4+73RNW/Gzgz6hAH3H10A4f+D+Axd58WLMB4O/DbptoTD5tKKhnYTTOqRCS5xPJVeQGRMY2FwBzgXnf/Sgz7jQPWuvv64Haz04CJjdS/GfhjYwcMriW5FHg5KHoGuD6GtsTFlpJK+mtgXESSTCyD4y8DVe5eC5EzCTPr5O6VTezXj8jdAusUAeMbqmhmA4HBwHtRxZlmtgAIAY+4+2tAd2Cvu4eijtkvhhha3P6qGkoqqjUwLiJJJ5bE8S7wOaDuzn8dgbeB81qwHZOAl+uSU2Cguxeb2UnAe2a2FIh5uRMzmwxMBsjLy6OwsLDR+uXl5U3WibapLNLU/ds2UFi4pYnarVdz424vFHdyUdwtK5bEkRl9u1h3LzezWL5mFwP9o7bzg7KGTAK+E13g7sXBz/VmVkhk/OMVINfMUoOzjqMe092nAFMAxo4d6wUFBY02trCwkKbqRHtr6Tb4x8dcdeHZjOqXE/N+rU1z424vFHdyUdwtK5YxjgozO6tuw8zGAAdi2G8+MDSYBZVOJDlMr1/JzIYBXYmMn9SVdTWzjOB5D+B8YIW7OzAbuDGoeivwegxtaXG6hkNEklUsZxzfA14ys61Ebh3bm8itZBvl7iEzuwuYCaQAU919uZk9BCxw97okMgmYFiSFOsOB35lZmEhyeyRqNta9wDQz+zmwCHgyhhha3KY9lXTLSqdzZloifr2ISMLEcgHg/OCs4NSgaJW718RycHefAcyoV/ZAve0HG9jvH8BpRznmeiIzthJKM6pEJFk12VVlZt8Bstx9mbsvA7LN7Nvxb1rrtqmkgoFKHCKShGIZ47gjuAMgAO5eCtwRtxa1ATW1YbburdLihiKSlGJJHCnRN3EKrghP6vukbt17gNqwM0AD4yKShGIZHP8r8Ccz+12w/U3grfg1qfXbFCynrq4qEUlGsSSOe4lcSHdnsL2EyMyqpLU5mIqrMw4RSUZNdlW5exiYB2wkMpvpUuDT+Darddu27wApHYy8zpmJboqIyAl31DMOMzuFyMKDNwO7gT8BuPslJ6ZprVdJRQ1dO6XToYM1XVlEpJ1prKtqJfABcK27rwUws3saqZ80Siuq6ZalC/9EJDk11lX1RWAbMNvMnjCzy4hcOZ70Siqrye2U1BPLRCSJHTVxuPtr7j4JGEZkfajvAb3M7LdmdsUJal+rtLeymm5KHCKSpGIZHK9w9xeCe4/nE1kf6t64t6wVK6mooWuWEoeIJKdm3Szb3UvdfYq7XxavBrV27k5pZTVdO2mMQ0SSU7MSh0BZVYjasNNNZxwikqSUOJppb2U1AF01xiEiSUqJo5lKKiKJQ2ccIpKslDiaqTQ448jVGIeIJCkljmYqrYjcw0pnHCKSrJQ4mqnujEPTcUUkWSlxNFNJRTWpHYzOGbEsLCwi0v4ocTRTaWUNuZ3Sibq3lYhIUolr4jCzCWa2yszWmtl9Dbz+mJktDh6rzWxvUD7azOaY2XIzW2JmX4ra52kz2xC13+h4xlCfFjgUkWQXt/6W4BazjwOXA0XAfDOb7u4r6uq4+z1R9e8Gzgw2K4Fb3H2NmfUFFprZzKh7n//Q3V+OV9sbowUORSTZxfOMYxyw1t3Xu3s1MA2Y2Ej9m4E/Arj7andfEzzfCuwEesaxrTErrdAChyKS3OI5wtsP2BK1XQSMb6iimQ0EBgPvNfDaOCAdWBdV/LCZPQC8C9zn7gcb2G8ykVvekpeXR2FhYaONLS8vb7IOwI69lfRLr4qpblsQa9ztjeJOLoq7ZbWWqUGTgJfdvTa60Mz6AM8Btwa3sAW4H9hOJJlMIbJS70P1D+juU4LXGTt2rBcUFDTagMLCQpqq4+5UvP0WI4cOpKBgWNNRtQGxxN0eKe7korhbVjy7qoqB/lHb+UFZQyYRdFPVMbMuwJvAv7j73Lpyd9/mEQeBp4h0iZ0QdQscap0qEUlm8Uwc84GhZjbYzNKJJIfp9SuZ2TCgKzAnqiwdeBV4tv4geHAWgkXmw14PLItXAPVpgUMRkTh2Vbl7yMzuAmYCKcBUd19uZg8BC9y9LolMAqa5u0ftfhNwEdDdzG4Lym5z98XAH8ysJ5Hb2C4G7oxXDPVpgUMRkTiPcbj7DGBGvbIH6m0/2MB+zwPPH+WYl7ZgE5tFCxyKiOjK8WbRAociIkoczaIFDkVElDiaRQsciogocTRLabDciBY4FJFkpsTRDKUVNVrgUESSnhJHM5RUVusaDhFJekoczVBaocQhIqLE0QyllTWaUSUiSU+JI0buTmmlbuIkIqLEESMtcCgiEqHEESMtcCgiEqHEESMtcCgiEqHEESMtcCgiEqHEEaMSLXAoIgIoccRsrxY4FBEBlDhipgUORUQilDhipAUORUQilDhipAUORUQilDhipAUORUQi4po4zGyCma0ys7Vmdl8Drz9mZouDx2oz2xv12q1mtiZ43BpVPsbMlgbH/JWdoL4jLXAoIhIRt5FeM0sBHgcuB4qA+WY23d1X1NVx93ui6t8NnBk87wb8FBgLOLAw2LcU+C1wBzAPmAFMAN6KVxx1SiurNaNKRIT4nnGMA9a6+3p3rwamARMbqX8z8Mfg+ZXALHcvCZLFLGCCmfUBurj7XHd34Fng+rhFEIgscKgxDhERiOMZB9AP2BK1XQSMb6iimQ0EBgPvNbJvv+BR1EB5Q8ecDEwGyMvLo7CwsNHGlpeXH7VORY1TG3b2bN1CYeH2Ro/T1jQWd3umuJOL4m5ZreWihEnAy+5e21IHdPcpwBSAsWPHekFBQaP1CwsLOVqdjbsr4N1Czj59OAVj8luqia1CY3G3Z4o7uSjulhXPrqpioH/Udn5Q1pBJfNZN1di+xcHzWI7ZYurWqdJyIyIi8U0c84GhZjbYzNKJJIfp9SuZ2TCgKzAnqngmcIWZdTWzrsAVwEx33waUmdk5wWyqW4DX4xgD8Fni0OC4iEgcu6rcPWRmdxFJAinAVHdfbmYPAQvcvS6JTAKmBYPddfuWmNm/EUk+AA+5e0nw/NvA00BHIrOp4j6jqm6Bw65aGVdEJL5jHO4+g8iU2eiyB+ptP3iUfacCUxsoXwCMarlWNk0LHIqIfEZXjsdACxyKiHxGiSMGWuBQROQzShwxKKmo1sV/IiIBJY4YlFbWaJ0qEZGAEkcMtMChiMhnlDhioAUORUQ+o8TRBC1wKCJyOCWOJpRVhagNu7qqREQCShxNKK0ILv5T4hARAZQ4mqQFDkVEDqfE0QQtcCgicjgljiZogUMRkcMpcTTh0BiHzjhERAAljiaVVmqBQxGRaEocTdAChyIih1PiaIIWOBQROZwSRxO0wKGIyOGUOJpQWlGtazhERKIocTShboxDREQi4po4zGyCma0ys7Vmdt9R6txkZivMbLmZvRCUXWJmi6MeVWZ2ffDa02a2Ieq10fFqvxY4FBE5UtzmmJpZCvA4cDlQBMw3s+nuviKqzlDgfuB8dy81s14A7j4bGB3U6QasBd6OOvwP3f3leLW9jhY4FBE5UjzPOMYBa919vbtXA9OAifXq3AE87u6lAO6+s4Hj3Ai85e6VcWxrg7TAoYjIkeKZOPoBW6K2i4KyaKcAp5jZh2Y218wmNHCcScAf65U9bGZLzOwxM8touSYfrkQLHIqIHCHRl0OnAkOBAiAfeN/MTnP3vQBm1gc4DZgZtc/9wHYgHZgC3As8VP/AZjYZmAyQl5dHYWFhow0pLy8/os7inSEANqxcim1PaV5kbURDcScDxZ1cFHfLimfiKAb6R23nB2XRioB57l4DbDCz1UQSyfzg9ZuAV4PXAXD3bcHTg2b2FPC/G/rl7j6FSGJh7NixXlBQ0GhjCwsLqV9n98Ii+PgTLrvwHAZ2z2p0/7aqobiTgeJOLoq7ZcWzq2o+MNTMBptZOpEup+n16rxG5GwDM+tBpOtqfdTrN1Ovmyo4C8Eia4BcDyxr+aZHaIFDEZEjxe2Mw91DZnYXkW6mFGCquy83s4eABe4+PXjtCjNbAdQSmS21B8DMBhE5Y/lbvUP/wcx6AgYsBu6MVwxa4FBE5Ehx/UR09xnAjHplD0Q9d+D7waP+vhs5cjAdd7+0xRt6FKWV1XTN0gKHIiLRdOV4I0oqqnUDJxGRetQH04jT83MZ3CM70c0QEWlVlDga8Z1LhiS6CSIirY66qkREpFmUOEREpFmUOEREpFmUOEREpFmUOEREpFmUOEREpFmUOEREpFmUOEREpFksslxU+2Zmu4BNTVTrAew+Ac1pbRR3clHcyeV44x7o7j3rFyZF4oiFmS1w97GJbseJpriTi+JOLvGKW11VIiLSLEocIiLSLEocn5mS6AYkiOJOLoo7ucQlbo1xiIhIs+iMQ0REmiXpE4eZTTCzVWa21szuS3R74snMpprZTjNbFlXWzcxmmdma4GfXRLaxpZlZfzObbWYrzGy5mX03KG/XcQOYWaaZfWRmnwSx/ywoH2xm84L3/J/MLD3RbW1pZpZiZovM7I1gu93HDGBmG81sqZktNrMFQVmLv9eTOnGYWQrwOHAVMAK42cxGJLZVcfU0MKFe2X3Au+4+FHg32G5PQsAP3H0EcA7wneBv3N7jBjgIXOruZwCjgQlmdg7wH8Bj7j4EKAVuT1wT4+a7wKdR28kQc51L3H101DTcFn+vJ3XiAMYBa919vbtXA9OAiQluU9y4+/tASb3iicAzwfNngOtPZJvizd23ufvHwfP9RD5M+tHO4wbwiPJgMy14OHAp8HJQ3u5iN7N84Brg98G20c5jbkKLv9eTPXH0A7ZEbRcFZckkz923Bc+3A3mJbEw8mdkg4ExgHkkSd9BlsxjYCcwC1gF73T0UVGmP7/lfAj8CwsF2d9p/zHUceNvMFprZ5KCsxd/ruue4HOLubmbtcpqdmWUDrwDfc/eyyJfQiPYct7vXAqPNLBd4FRiW2BbFl5ldC+x094VmVpDg5iTCBe5ebGa9gFlmtjL6xZZ6ryf7GUcx0D9qOz8oSyY7zKwPQPBzZ4Lb0+LMLI1I0viDu/85KG73cUdz973AbOBcINfM6r40trf3/PnA581sI5Gu50uB/6Z9x3yIuxcHP3cS+aIwjji815M9ccwHhgYzLtKBScD0BLfpRJsO3Bo8vxV4PYFtaXFB//aTwKfu/l9RL7XruAHMrGdwpoGZdQQuJzLGMxu4MajWrmJ39/vdPd/dBxH5//yeu3+ZdhxzHTPLMrPOdc+BK4BlxOG9nvQXAJrZ1UT6RFOAqe7+cGJbFD9m9keggMiKmTuAnwKvAS8CA4isIHyTu9cfQG+zzOwC4ANgKZ/1ef+YyDhHu40bwMxOJzIYmkLkS+KL7v6QmZ1E5Nt4N2AR8BV3P5i4lsZH0FX1v9392mSIOYjx1WAzFXjB3R82s+608Hs96ROHiIg0T7J3VYmISDMpcYiISLMocYiISLMocYiISLMocYiISLMocYi0ADOrDVYkrXu02KKJZjYoekVjkUTTkiMiLeOAu49OdCNETgSdcYjEUXB/hP8T3CPhIzMbEpQPMrP3zGyJmb1rZgOC8jwzezW4h8YnZnZecKgUM3siuK/G28GV4CIJocQh0jI61uuq+lLUa/vc/TTgN0RWKQD4NfCMu58O/AH4VVD+K+BvwT00zgKWB+VDgcfdfSSwF7ghrtGINEJXjou0ADMrd/fsBso3ErmZ0vpgscXt7t7dzHYDfdy9Jijf5u49zGwXkB+9HEawHPys4EY8mNm9QJq7//wEhCZyBJ1xiMSfH+V5c0Svq1SLxiclgZQ4ROLvS1E/5wTP/0Fk9VaALxNZiBEit/b8Fhy6CVPOiWqkSKz0rUWkZXQM7rRX56/uXjclt6uZLSFy1nBzUHY38JSZ/RDYBXwtKP8uMMXMbidyZvEtYBsirYjGOETiKBjjGOvuuxPdFpGWoq4qERFpFp1xiIhIs+iMQ0REmkWJQ0REmkWJQ0REmkWJQ0REmkWJQ0REmkWJQ0REmuX/A0sSidI3PfNcAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 训练模型\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRY, N_LAYER)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "def trainModel():\n",
    "    def time_since(since):\n",
    "        s = time.time() - since\n",
    "        m = math.floor(s / 60)\n",
    "        s -= m * 60\n",
    "        return '%dm %ds' % (m, s)\n",
    "\n",
    "    total_loss = 0\n",
    "    for i, (names, countries) in enumerate(trainLoader, 1):\n",
    "        # print(type(names), type(countries))\n",
    "        # print(len(names), countries.shape)\n",
    "        inputs, seq_lengths, target = make_tensors(names, countries)\n",
    "        inputs = inputs\n",
    "        seq_lengths = seq_lengths\n",
    "        target = target\n",
    "        output = classifier(inputs, seq_lengths)\n",
    "        # print(\"Shape:\", output.shape, target.shape)\n",
    "        # 注意输出和目标的维度：Shape: torch.Size([256, 18]) torch.Size([256])\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print(f'[{time_since(start)}] Epoch {epoch} ', end='')\n",
    "            print(f'[{i * len(inputs)}/{len(trainSet)}] ', end='')\n",
    "            print(f'loss={total_loss / (i * len(inputs))}')\n",
    "    return total_loss\n",
    "\n",
    "def testModel():\n",
    "    correct = 0\n",
    "    total = len(testSet)\n",
    "    print(\"evaluating trained model ... \")\n",
    "    with torch.no_grad():\n",
    "        for i, (names, countries) in enumerate(testLoader):\n",
    "            inputs, seq_lengths, target = make_tensors(names, countries)\n",
    "            inputs = inputs\n",
    "            seq_lengths = seq_lengths\n",
    "            target = target\n",
    "            output = classifier(inputs, seq_lengths)\n",
    "            # 注意这个keepdim的使用，为了直接和target计算loss\n",
    "            pred = output.max(dim=1, keepdim=True)[1]\n",
    "            # 注意这个view_as 和 eq\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        percent = '%.2f' % (100 * correct / total)\n",
    "        print(f'Test set: Accuracy {correct}/{total} {percent}%')\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "N_EPOCHS = 50\n",
    "start = time.time()\n",
    "print(\"Training for %d epochs...\" % N_EPOCHS)\n",
    "acc_list = []\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    # Train cycle\n",
    "    total_loss = trainModel()\n",
    "    acc = testModel()\n",
    "    acc_list.append(acc)\n",
    "\n",
    "\n",
    "# 可视化\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "epoch = np.arange(1, len(acc_list) + 1)\n",
    "acc_list = np.array(acc_list)\n",
    "plt.plot(epoch, acc_list)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}