{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "w = 1.0\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return x * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_funtion(xs, ys):\n",
    "    cost = 0\n",
    "    for x, y in zip(xs, ys):\n",
    "        pre_y = forward(x)\n",
    "        cost += (pre_y - y) ** 2\n",
    "    return cost / len(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(xs, ys):\n",
    "    grad = 0\n",
    "    for x, y in zip(xs, ys):\n",
    "        grad += (2 * x * (x * w - y))\n",
    "    return grad / len(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "开始预测4的值： 7.999777758621207\nepoch: 0 w: 1.9999496252874736 grad_val: -0.0005185632171844645\nepoch: 1 w: 1.9999543269273095 grad_val: -0.0004701639835797226\nepoch: 2 w: 1.9999585897474272 grad_val: -0.00042628201177752345\nepoch: 3 w: 1.999962454704334 grad_val: -0.00038649569067909323\nepoch: 4 w: 1.9999659589319294 grad_val: -0.00035042275954907015\nepoch: 5 w: 1.9999691360982828 grad_val: -0.00031771663532461264\nepoch: 6 w: 1.9999720167291097 grad_val: -0.0002880630826947635\nepoch: 7 w: 1.9999746285010596 grad_val: -0.0002611771949766843\nepoch: 8 w: 1.9999769965076273 grad_val: -0.00023680065677655904\nepoch: 9 w: 1.9999791435002487 grad_val: -0.00021469926214449467\nepoch: 10 w: 1.9999810901068922 grad_val: -0.0001946606643450366\nepoch: 11 w: 1.999982855030249 grad_val: -0.00017649233567388714\nepoch: 12 w: 1.9999844552274257 grad_val: -0.0001600197176753563\nepoch: 13 w: 1.999985906072866 grad_val: -0.00014508454402687812\nepoch: 14 w: 1.9999872215060652 grad_val: -0.00013154331991804824\nepoch: 15 w: 1.999988414165499 grad_val: -0.00011926594339077094\nepoch: 16 w: 1.9999894955100526 grad_val: -0.00010813445534211634\nepoch: 17 w: 1.9999904759291143 grad_val: -9.804190617653437e-05\nepoch: 18 w: 1.999991364842397 grad_val: -8.889132826676398e-05\nepoch: 19 w: 1.99999217079044 grad_val: -8.059480429389865e-05\nepoch: 20 w: 1.9999929015166655 grad_val: -7.307262256039948e-05\nepoch: 21 w: 1.9999935640417768 grad_val: -6.62525111216894e-05\nepoch: 22 w: 1.999994164731211 grad_val: -6.006894341699839e-05\nepoch: 23 w: 1.999994709356298 grad_val: -5.446250869785748e-05\nepoch: 24 w: 1.9999952031497101 grad_val: -4.9379341219365123e-05\nepoch: 25 w: 1.9999956508557373 grad_val: -4.477060270631957e-05\nepoch: 26 w: 1.9999960567758686 grad_val: -4.059201311923388e-05\nepoch: 27 w: 1.9999964248101207 grad_val: -3.6803425227234975e-05\nepoch: 28 w: 1.9999967584945095 grad_val: -3.33684388730641e-05\nepoch: 29 w: 1.999997061035022 grad_val: -3.0254051243942353e-05\nepoch: 30 w: 1.99999733533842 grad_val: -2.743033979513143e-05\nepoch: 31 w: 1.9999975840401674 grad_val: -2.4870174746727258e-05\nepoch: 32 w: 1.9999978095297517 grad_val: -2.2548958438820914e-05\nepoch: 33 w: 1.9999980139736415 grad_val: -2.044438898500071e-05\nepoch: 34 w: 1.9999981993361016 grad_val: -1.8536246012483087e-05\nepoch: 35 w: 1.9999983673980655 grad_val: -1.680619638448159e-05\nepoch: 36 w: 1.999998519774246 grad_val: -1.5237618055626475e-05\nepoch: 37 w: 1.9999986579286497 grad_val: -1.3815440371267584e-05\nepoch: 38 w: 1.9999987831886423 grad_val: -1.2525999269670981e-05\nepoch: 39 w: 1.9999988967577025 grad_val: -1.1356906005399736e-05\nepoch: 40 w: 1.9999989997269836 grad_val: -1.0296928110165027e-05\nepoch: 41 w: 1.9999990930857985 grad_val: -9.335881486549624e-06\nepoch: 42 w: 1.999999177731124 grad_val: -8.464532547473405e-06\nepoch: 43 w: 1.999999254476219 grad_val: -7.674509510261865e-06\nepoch: 44 w: 1.9999993240584386 grad_val: -6.958221954474671e-06\nepoch: 45 w: 1.9999993871463178 grad_val: -6.308787906306179e-06\nepoch: 46 w: 1.9999994443459947 grad_val: -5.719967700527444e-06\nepoch: 47 w: 1.9999994962070353 grad_val: -5.186104048574928e-06\nepoch: 48 w: 1.999999543227712 grad_val: -4.7020676703368736e-06\nepoch: 49 w: 1.9999995858597923 grad_val: -4.263208022159404e-06\nepoch: 50 w: 1.9999996245128784 grad_val: -3.865308605484803e-06\nepoch: 51 w: 1.9999996595583431 grad_val: -3.504546468876176e-06\nepoch: 52 w: 1.9999996913328977 grad_val: -3.1774554636321284e-06\nepoch: 53 w: 1.9999997201418271 grad_val: -2.8808929545635444e-06\nepoch: 54 w: 1.9999997462619232 grad_val: -2.6120096128975e-06\nepoch: 55 w: 1.9999997699441436 grad_val: -2.368222049871823e-06\nepoch: 56 w: 1.9999997914160235 grad_val: -2.147187993184474e-06\nepoch: 57 w: 1.9999998108838613 grad_val: -1.9467837805523893e-06\nepoch: 58 w: 1.999999828534701 grad_val: -1.765083961657865e-06\nepoch: 59 w: 1.9999998445381288 grad_val: -1.6003427917669437e-06\nepoch: 60 w: 1.9999998590479036 grad_val: -1.4509774645491784e-06\nepoch: 61 w: 1.9999998722034327 grad_val: -1.315552900541898e-06\nepoch: 62 w: 1.9999998841311122 grad_val: -1.1927679621770437e-06\nepoch: 63 w: 1.9999998949455418 grad_val: -1.081442952965972e-06\nepoch: 64 w: 1.9999999047506245 grad_val: -9.805082764972421e-07\nepoch: 65 w: 1.9999999136405662 grad_val: -8.889941713145314e-07\nepoch: 66 w: 1.99999992170078 grad_val: -8.060213832668713e-07\nepoch: 67 w: 1.9999999290087072 grad_val: -7.307927208562622e-07\nepoch: 68 w: 1.9999999356345612 grad_val: -6.625853998798448e-07\nepoch: 69 w: 1.9999999416420022 grad_val: -6.007440941739143e-07\nepoch: 70 w: 1.9999999470887486 grad_val: -5.446746462745011e-07\nepoch: 71 w: 1.999999952027132 grad_val: -4.9383834621608e-07\nepoch: 72 w: 1.9999999565045998 grad_val: -4.477467661819408e-07\nepoch: 73 w: 1.9999999605641705 grad_val: -4.059570682575971e-07\nepoch: 74 w: 1.9999999642448478 grad_val: -3.6806774147043103e-07\nepoch: 75 w: 1.9999999675819953 grad_val: -3.3371475449683885e-07\nepoch: 76 w: 1.9999999706076756 grad_val: -3.025680449889971e-07\nepoch: 77 w: 1.9999999733509592 grad_val: -2.7432836106437247e-07\nepoch: 78 w: 1.999999975838203 grad_val: -2.4872438123916635e-07\nepoch: 79 w: 1.999999978093304 grad_val: -2.255101054042067e-07\nepoch: 80 w: 1.999999980137929 grad_val: -2.0446249611912512e-07\nepoch: 81 w: 1.9999999819917222 grad_val: -1.853793302567889e-07\nepoch: 82 w: 1.9999999836724949 grad_val: -1.6807725951769234e-07\nepoch: 83 w: 1.9999999851963954 grad_val: -1.5239004872806086e-07\nepoch: 84 w: 1.999999986578065 grad_val: -1.38166975934458e-07\nepoch: 85 w: 1.999999987830779 grad_val: -1.2527139281459654e-07\nepoch: 86 w: 1.999999988966573 grad_val: -1.1357939522227412e-07\nepoch: 87 w: 1.9999999899963594 grad_val: -1.0297865345639441e-07\nepoch: 88 w: 1.9999999909300326 grad_val: -9.336731210396465e-08\nepoch: 89 w: 1.9999999917765627 grad_val: -8.465302858695622e-08\nepoch: 90 w: 1.9999999925440834 grad_val: -7.67520815792011e-08\nepoch: 91 w: 1.999999993239969 grad_val: -6.958855403027542e-08\nepoch: 92 w: 1.9999999938709052 grad_val: -6.309362291882319e-08\nepoch: 93 w: 1.999999994442954 grad_val: -5.720488512513574e-08\nepoch: 94 w: 1.9999999949616116 grad_val: -5.186576116737266e-08\nepoch: 95 w: 1.9999999954318612 grad_val: -4.702495814967733e-08\nepoch: 96 w: 1.9999999958582209 grad_val: -4.263596172412084e-08\nepoch: 97 w: 1.999999996244787 grad_val: -3.8656604880079236e-08\nepoch: 98 w: 1.9999999965952735 grad_val: -3.5048654941268374e-08\nepoch: 99 w: 1.999999996913048 grad_val: -3.177744778426472e-08\nepoch: 100 w: 1.9999999972011635 grad_val: -2.8811552255092465e-08\nepoch: 101 w: 1.9999999974623883 grad_val: -2.6122473517631306e-08\nepoch: 102 w: 1.999999997699232 grad_val: -2.368437520180085e-08\nepoch: 103 w: 1.9999999979139704 grad_val: -2.147383574661414e-08\nepoch: 104 w: 1.9999999981086665 grad_val: -1.9469610220331408e-08\nepoch: 105 w: 1.999999998285191 grad_val: -1.765244676358672e-08\nepoch: 106 w: 1.99999999844524 grad_val: -1.6004884084708237e-08\nepoch: 107 w: 1.999999998590351 grad_val: -1.451109469622717e-08\nepoch: 108 w: 1.9999999987219181 grad_val: -1.3156724154583799e-08\nepoch: 109 w: 1.9999999988412058 grad_val: -1.1928764494702667e-08\nepoch: 110 w: 1.99999999894936 grad_val: -1.081541271356438e-08\nepoch: 111 w: 1.9999999990474198 grad_val: -9.805972626490226e-09\nepoch: 112 w: 1.9999999991363273 grad_val: -8.89074887785076e-09\nepoch: 113 w: 1.9999999992169368 grad_val: -8.060945096607005e-09\nepoch: 114 w: 1.9999999992900228 grad_val: -7.308589141293472e-09\nepoch: 115 w: 1.9999999993562874 grad_val: -6.626454200168534e-09\nepoch: 116 w: 1.9999999994163673 grad_val: -6.007984628316383e-09\nepoch: 117 w: 1.9999999994708397 grad_val: -5.4472385121092275e-09\nepoch: 118 w: 1.999999999520228 grad_val: -4.9388283092829015e-09\nepoch: 119 w: 1.9999999995650066 grad_val: -4.4778724432129975e-09\nepoch: 120 w: 1.999999999605606 grad_val: -4.059938163436527e-09\nepoch: 121 w: 1.999999999642416 grad_val: -3.6810117916748672e-09\nepoch: 122 w: 1.9999999996757905 grad_val: -3.3374498720206702e-09\nepoch: 123 w: 1.99999999970605 grad_val: -3.0259548940610634e-09\nepoch: 124 w: 1.9999999997334854 grad_val: -2.74353354849192e-09\nepoch: 125 w: 1.9999999997583602 grad_val: -2.487469045557115e-09\nepoch: 126 w: 1.9999999997809133 grad_val: -2.25530542389644e-09\nepoch: 127 w: 1.9999999998013613 grad_val: -2.044809210843823e-09\nepoch: 128 w: 1.999999999819901 grad_val: -1.8539612807918122e-09\nepoch: 129 w: 1.9999999998367102 grad_val: -1.6809235485008382e-09\nepoch: 130 w: 1.9999999998519506 grad_val: -1.5240383769802672e-09\nepoch: 131 w: 1.9999999998657685 grad_val: -1.3817934944408232e-09\nepoch: 132 w: 1.9999999998782967 grad_val: -1.2528279154840523e-09\nepoch: 133 w: 1.9999999998896556 grad_val: -1.1358971541142182e-09\nepoch: 134 w: 1.9999999998999545 grad_val: -1.0298806252251325e-09\nepoch: 135 w: 1.9999999999092921 grad_val: -9.337586999909793e-10\nepoch: 136 w: 1.9999999999177582 grad_val: -8.466069327065876e-10\nepoch: 137 w: 1.999999999925434 grad_val: -7.675898435384928e-10\nepoch: 138 w: 1.9999999999323936 grad_val: -6.959480399378511e-10\nepoch: 139 w: 1.9999999999387035 grad_val: -6.309927395401852e-10\nepoch: 140 w: 1.9999999999444245 grad_val: -5.721004410948657e-10\nepoch: 141 w: 1.9999999999496114 grad_val: -5.187059670674898e-10\nepoch: 142 w: 1.9999999999543143 grad_val: -4.702943220043684e-10\nepoch: 143 w: 1.9999999999585782 grad_val: -4.2639876814594874e-10\nepoch: 144 w: 1.9999999999624443 grad_val: -3.866026017836551e-10\nepoch: 145 w: 1.9999999999659495 grad_val: -3.505202054536009e-10\nepoch: 146 w: 1.9999999999691276 grad_val: -3.178041533639468e-10\nepoch: 147 w: 1.999999999972009 grad_val: -2.8814299094885126e-10\nepoch: 148 w: 1.9999999999746216 grad_val: -2.6124880037059484e-10\nepoch: 149 w: 1.9999999999769902 grad_val: -2.368638618577279e-10\nepoch: 150 w: 1.9999999999791378 grad_val: -2.1475784114007487e-10\nepoch: 151 w: 1.999999999981085 grad_val: -1.9471343056428245e-10\nepoch: 152 w: 1.9999999999828504 grad_val: -1.7654174418642773e-10\nepoch: 153 w: 1.999999999984451 grad_val: -1.6006262981704822e-10\nepoch: 154 w: 1.9999999999859024 grad_val: -1.4512391288690196e-10\nepoch: 155 w: 1.9999999999872182 grad_val: -1.315785998675286e-10\nepoch: 156 w: 1.9999999999884113 grad_val: -1.1929657262044202e-10\nepoch: 157 w: 1.9999999999894928 grad_val: -1.081610356834517e-10\nepoch: 158 w: 1.9999999999904736 grad_val: -9.806673991382316e-11\nepoch: 159 w: 1.9999999999913627 grad_val: -8.891243297171059e-11\nepoch: 160 w: 1.999999999992169 grad_val: -8.061447805592555e-11\nepoch: 161 w: 1.9999999999929 grad_val: -7.30899785139627e-11\nepoch: 162 w: 1.9999999999935625 grad_val: -6.626758401277281e-11\nepoch: 163 w: 1.9999999999941633 grad_val: -6.008349373587407e-11\nepoch: 164 w: 1.999999999994708 grad_val: -5.447464701546778e-11\nepoch: 165 w: 1.9999999999952018 grad_val: -4.939234206820705e-11\nepoch: 166 w: 1.9999999999956497 grad_val: -4.4783288188909864e-11\nepoch: 167 w: 1.9999999999960558 grad_val: -4.060278039711799e-11\nepoch: 168 w: 1.999999999996424 grad_val: -3.681188687210124e-11\nepoch: 169 w: 1.9999999999967577 grad_val: -3.337670880417439e-11\nepoch: 170 w: 1.9999999999970604 grad_val: -3.02623111754959e-11\nepoch: 171 w: 1.9999999999973348 grad_val: -2.743627547374672e-11\nepoch: 172 w: 1.9999999999975835 grad_val: -2.4874768911331557e-11\nepoch: 173 w: 1.999999999997809 grad_val: -2.2553662641181898e-11\nepoch: 174 w: 1.9999999999980136 grad_val: -2.0448087667546133e-11\nepoch: 175 w: 1.999999999998199 grad_val: -1.8540724511240114e-11\nepoch: 176 w: 1.999999999998367 grad_val: -1.6808924622561488e-11\nepoch: 177 w: 1.9999999999985194 grad_val: -1.524143774152738e-11\nepoch: 178 w: 1.9999999999986575 grad_val: -1.3818871972641015e-11\nepoch: 179 w: 1.9999999999987828 grad_val: -1.25289408477632e-11\nepoch: 180 w: 1.9999999999988964 grad_val: -1.1361874404277236e-11\nepoch: 181 w: 1.9999999999989995 grad_val: -1.0300797252208819e-11\nepoch: 182 w: 1.999999999999093 grad_val: -9.339196083146817e-12\nepoch: 183 w: 1.9999999999991775 grad_val: -8.46626472631821e-12\nepoch: 184 w: 1.9999999999992544 grad_val: -7.676230021994948e-12\nepoch: 185 w: 1.9999999999993239 grad_val: -6.958285799404014e-12\nepoch: 186 w: 1.999999999999387 grad_val: -6.31095176117924e-12\nepoch: 187 w: 1.9999999999994442 grad_val: -5.72238552839129e-12\nepoch: 188 w: 1.9999999999994962 grad_val: -5.186813941312114e-12\nepoch: 189 w: 1.9999999999995433 grad_val: -4.702756702575546e-12\nepoch: 190 w: 1.9999999999995859 grad_val: -4.263404444297218e-12\nepoch: 191 w: 1.9999999999996245 grad_val: -3.865500512271562e-12\nepoch: 192 w: 1.9999999999996596 grad_val: -3.5040118954536106e-12\nepoch: 193 w: 1.9999999999996914 grad_val: -3.177458296477198e-12\nepoch: 194 w: 1.9999999999997202 grad_val: -2.8797704961410395e-12\nepoch: 195 w: 1.9999999999997464 grad_val: -2.611244553918368e-12\nepoch: 196 w: 1.9999999999997702 grad_val: -2.3658112506079e-12\nepoch: 197 w: 1.9999999999997917 grad_val: -2.1445067943659524e-12\nepoch: 198 w: 1.9999999999998113 grad_val: -1.9448146796700407e-12\nepoch: 199 w: 1.9999999999998288 grad_val: -1.7624420441582818e-12\n预测到4的值为： 7.999999999999315\n"
     ]
    }
   ],
   "source": [
    "print(\"开始预测4的值：\", forward(4))\n",
    "for epoch in range(200):\n",
    "    cost_val = loss_funtion(x_data, y_data)\n",
    "    grad_val = gradient(x_data, y_data)\n",
    "    w -= lr * grad_val\n",
    "    print(\"epoch:\", epoch, \"w:\", w, \"grad_val:\", grad_val)\n",
    "print(\"预测到4的值为：\", forward(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "d\t 3.0 6.0 -1.8153673408366782\n",
      "progress: 81 w = 1.9009616261832436 loss= 0.08827739539440821\n",
      "grad\t 1.0 2.0 -0.1980767476335128\n",
      "grad\t 2.0 4.0 -0.7907223765529832\n",
      "grad\t 3.0 6.0 -1.7648923444662579\n",
      "progress: 82 w = 1.9037153176518964 loss= 0.08343666049387669\n",
      "grad\t 1.0 2.0 -0.19256936469620722\n",
      "grad\t 2.0 4.0 -0.7687369038672589\n",
      "grad\t 3.0 6.0 -1.7158207694317227\n",
      "progress: 83 w = 1.9063924446898917 loss= 0.0788613697002148\n",
      "grad\t 1.0 2.0 -0.18721511062021667\n",
      "grad\t 2.0 4.0 -0.7473627215959056\n",
      "grad\t 3.0 6.0 -1.6681135946020582\n",
      "progress: 84 w = 1.9089951361167097 loss= 0.07453696725374591\n",
      "grad\t 1.0 2.0 -0.18200972776658064\n",
      "grad\t 2.0 4.0 -0.7265828332441906\n",
      "grad\t 3.0 6.0 -1.6217328838010374\n",
      "progress: 85 w = 1.9115254615615214 loss= 0.07044969556711662\n",
      "grad\t 1.0 2.0 -0.17694907687695727\n",
      "grad\t 2.0 4.0 -0.7063807148928127\n",
      "grad\t 3.0 6.0 -1.5766417556407593\n",
      "progress: 86 w = 1.9139854331089319 loss= 0.0665865514571224\n",
      "grad\t 1.0 2.0 -0.17202913378213625\n",
      "grad\t 2.0 4.0 -0.6867403020582881\n",
      "grad\t 3.0 6.0 -1.5328043541941003\n",
      "progress: 87 w = 1.9163770068989663 loss= 0.0629352447765799\n",
      "grad\t 1.0 2.0 -0.16724598620206743\n",
      "grad\t 2.0 4.0 -0.6676459769186529\n",
      "grad\t 3.0 6.0 -1.4901858204824343\n",
      "progress: 88 w = 1.9187020846825693 loss= 0.05948415931464108\n",
      "grad\t 1.0 2.0 -0.16259583063486138\n",
      "grad\t 2.0 4.0 -0.6490825558943669\n",
      "grad\t 3.0 6.0 -1.4487522647562265\n",
      "progress: 89 w = 1.9209625153338548 loss= 0.0562223158411604\n",
      "grad\t 1.0 2.0 -0.15807496933229048\n",
      "grad\t 2.0 4.0 -0.6310352775745027\n",
      "grad\t 3.0 6.0 -1.408470739546292\n",
      "progress: 90 w = 1.923160096320308 loss= 0.053139337177538985\n",
      "grad\t 1.0 2.0 -0.15367980735938414\n",
      "grad\t 2.0 4.0 -0.6134897909786616\n",
      "grad\t 3.0 6.0 -1.3693092134643745\n",
      "progress: 91 w = 1.9252965751321103 loss= 0.050225415182932005\n",
      "grad\t 1.0 2.0 -0.14940684973577945\n",
      "grad\t 2.0 4.0 -0.5964321441452309\n",
      "grad\t 3.0 6.0 -1.331236545732155\n",
      "progress: 92 w = 1.9273736506717234 loss= 0.04747127955077597\n",
      "grad\t 1.0 2.0 -0.14525269865655321\n",
      "grad\t 2.0 4.0 -0.5798487730369608\n",
      "grad\t 3.0 6.0 -1.2942224614184958\n",
      "progress: 93 w = 1.9293929746048353 loss= 0.04486816831638083\n",
      "grad\t 1.0 2.0 -0.14121405079032945\n",
      "grad\t 2.0 4.0 -0.5637264907549948\n",
      "grad\t 3.0 6.0 -1.25823752736515\n",
      "progress: 94 w = 1.9313561526737457 loss= 0.04240779998175096\n",
      "grad\t 1.0 2.0 -0.13728769465250856\n",
      "grad\t 2.0 4.0 -0.5480524770528135\n",
      "grad\t 3.0 6.0 -1.223253128781879\n",
      "progress: 95 w = 1.9332647459742331 loss= 0.04008234716895259\n",
      "grad\t 1.0 2.0 -0.1334705080515337\n",
      "grad\t 2.0 4.0 -0.5328142681417223\n",
      "grad\t 3.0 6.0 -1.189241446492323\n",
      "progress: 96 w = 1.9351202721969187 loss= 0.03788441171821725\n",
      "grad\t 1.0 2.0 -0.12975945560616253\n",
      "grad\t 2.0 4.0 -0.5179997467798003\n",
      "grad\t 3.0 6.0 -1.1561754348125124\n",
      "progress: 97 w = 1.9369242068341175 loss= 0.03580700115154665\n",
      "grad\t 1.0 2.0 -0.12615158633176504\n",
      "grad\t 2.0 4.0 -0.5035971326364059\n",
      "grad\t 3.0 6.0 -1.12402880004446\n",
      "progress: 98 w = 1.93867798435313 loss= 0.03384350642695453\n",
      "grad\t 1.0 2.0 -0.1226440312937398\n",
      "grad\t 2.0 4.0 -0.48959497292460874\n",
      "grad\t 3.0 6.0 -1.0927759795677243\n",
      "progress: 99 w = 1.9403829993369164 loss= 0.03198768091255917\n",
      "grad\t 1.0 2.0 -0.11923400132616724\n",
      "grad\t 2.0 4.0 -0.47598213329406036\n",
      "grad\t 3.0 6.0 -1.062392121512346\n",
      "progress: 100 w = 1.9420406075930488 loss= 0.03023362051364647\n",
      "grad\t 1.0 2.0 -0.1159187848139025\n",
      "grad\t 2.0 4.0 -0.462747788977099\n",
      "grad\t 3.0 6.0 -1.0328530649968855\n",
      "progress: 101 w = 1.9436521272318366 loss= 0.028575744889474293\n",
      "grad\t 1.0 2.0 -0.11269574553632689\n",
      "grad\t 2.0 4.0 -0.4498814161810163\n",
      "grad\t 3.0 6.0 -1.004135320916026\n",
      "progress: 102 w = 1.94521883971447 loss= 0.027008779700060295\n",
      "grad\t 1.0 2.0 -0.10956232057105986\n",
      "grad\t 2.0 4.0 -0.43737278371967037\n",
      "grad\t 3.0 6.0 -0.9762160532623039\n",
      "progress: 103 w = 1.9467419908720232 loss= 0.025527739826480916\n",
      "grad\t 1.0 2.0 -0.10651601825595369\n",
      "grad\t 2.0 4.0 -0.42521194487776626\n",
      "grad\t 3.0 6.0 -0.9490730609671729\n",
      "progress: 104 w = 1.9482227918961241 loss= 0.024127913511288623\n",
      "grad\t 1.0 2.0 -0.10355441620775174\n",
      "grad\t 2.0 4.0 -0.4133892295013446\n",
      "grad\t 3.0 6.0 -0.9226847602470034\n",
      "progress: 105 w = 1.9496624203020803 loss= 0.022804847368599796\n",
      "grad\t 1.0 2.0 -0.10067515939583949\n",
      "grad\t 2.0 4.0 -0.40189523630819046\n",
      "grad\t 3.0 6.0 -0.897030167439878\n",
      "progress: 106 w = 1.9510620208652243 loss= 0.02155433221616175\n",
      "grad\t 1.0 2.0 -0.09787595826955142\n",
      "grad\t 2.0 4.0 -0.3907208254120498\n",
      "grad\t 3.0 6.0 -0.8720888823196908\n",
      "progress: 107 w = 1.9524227065312256 loss= 0.02037238968432499\n",
      "grad\t 1.0 2.0 -0.09515458693754875\n",
      "grad\t 2.0 4.0 -0.3798571110546938\n",
      "grad\t 3.0 6.0 -0.847841071874079\n",
      "progress: 108 w = 1.9537455593010922 loss= 0.01925525955931903\n",
      "grad\t 1.0 2.0 -0.09250888139781566\n",
      "grad\t 2.0 4.0 -0.3692954545400795\n",
      "grad\t 3.0 6.0 -0.8242674545334587\n",
      "progress: 109 w = 1.9550316310915636 loss= 0.018199387820567094\n",
      "grad\t 1.0 2.0 -0.08993673781687272\n",
      "grad\t 2.0 4.0 -0.3590274573649559\n",
      "grad\t 3.0 6.0 -0.8013492848385813\n",
      "progress: 110 w = 1.9562819445715842 loss= 0.01720141533397831\n",
      "grad\t 1.0 2.0 -0.08743611085683156\n",
      "grad\t 2.0 4.0 -0.3490449545404708\n",
      "grad\t 3.0 6.0 -0.7790683385343335\n",
      "progress: 111 w = 1.9574974939755159 loss= 0.016258167165251733\n",
      "grad\t 1.0 2.0 -0.08500501204896826\n",
      "grad\t 2.0 4.0 -0.33934000809948195\n",
      "grad\t 3.0 6.0 -0.7574068980780453\n",
      "progress: 112 w = 1.9586792458937423 loss= 0.015366642479188411\n",
      "grad\t 1.0 2.0 -0.08264150821251537\n",
      "grad\t 2.0 4.0 -0.32990490078436174\n",
      "grad\t 3.0 6.0 -0.7363477385506947\n",
      "progress: 113 w = 1.9598281400412898 loss= 0.01452400499287996\n",
      "grad\t 1.0 2.0 -0.0803437199174204\n",
      "grad\t 2.0 4.0 -0.3207321299103416\n",
      "grad\t 3.0 6.0 -0.715874113959881\n",
      "progress: 114 w = 1.9609450900050776 loss= 0.013727573952403493\n",
      "grad\t 1.0 2.0 -0.0781098199898449\n",
      "grad\t 2.0 4.0 -0.31181440139946126\n",
      "grad\t 3.0 6.0 -0.6959697439236017\n",
      "progress: 115 w = 1.9620309839703902 loss= 0.01297481560431083\n",
      "grad\t 1.0 2.0 -0.07593803205921956\n",
      "grad\t 2.0 4.0 -0.30314462398040476\n",
      "grad\t 3.0 6.0 -0.6766188007242615\n",
      "progress: 116 w = 1.9630866854271543 loss= 0.012263335134784696\n",
      "grad\t 1.0 2.0 -0.07382662914569149\n",
      "grad\t 2.0 4.0 -0.29471590354959964\n",
      "grad\t 3.0 6.0 -0.6578058967227083\n",
      "progress: 117 w = 1.9641130338565724 loss= 0.011590869050815553\n",
      "grad\t 1.0 2.0 -0.07177393228685514\n",
      "grad\t 2.0 4.0 -0.2865215376891257\n",
      "grad\t 3.0 6.0 -0.6395160721221274\n",
      "progress: 118 w = 1.9651108453986705 loss= 0.010955277979159173\n",
      "grad\t 1.0 2.0 -0.06977830920265893\n",
      "grad\t 2.0 4.0 -0.2785550103370138\n",
      "grad\t 3.0 6.0 -0.6217347830722133\n",
      "progress: 119 w = 1.9660809135012824 loss= 0.010354539860167352\n",
      "grad\t 1.0 2.0 -0.06783817299743511\n",
      "grad\t 2.0 4.0 -0.2708099866057605\n",
      "grad\t 3.0 6.0 -0.6044478901040584\n",
      "progress: 120 w = 1.9670240095509899 loss= 0.009786743514838814\n",
      "grad\t 1.0 2.0 -0.06595198089802023\n",
      "grad\t 2.0 4.0 -0.2632803077448962\n",
      "grad\t 3.0 6.0 -0.5876416468866079\n",
      "progress: 121 w = 1.9679408834865195 loss= 0.009250082564624337\n",
      "grad\t 1.0 2.0 -0.06411823302696096\n",
      "grad\t 2.0 4.0 -0.2559599862436279\n",
      "grad\t 3.0 6.0 -0.5713026892957771\n",
      "progress: 122 w = 1.968832264395086 loss= 0.008742849684640477\n",
      "grad\t 1.0 2.0 -0.062335471209828075\n",
      "grad\t 2.0 4.0 -0.24884320106963287\n",
      "grad\t 3.0 6.0 -0.5554180247874214\n",
      "progress: 123 w = 1.9696988610921529 loss= 0.008263431172013908\n",
      "grad\t 1.0 2.0 -0.06060227781569427\n",
      "grad\t 2.0 4.0 -0.24192429304025076\n",
      "grad\t 3.0 6.0 -0.5399750220658408\n",
      "progress: 124 w = 1.9705413626850747 loss= 0.0078103018120707635\n",
      "grad\t 1.0 2.0 -0.058917274629850525\n",
      "grad\t 2.0 4.0 -0.23519776032236273\n",
      "grad\t 3.0 6.0 -0.5249614010395156\n",
      "progress: 125 w = 1.9713604391210664 loss= 0.0073820200260432955\n",
      "grad\t 1.0 2.0 -0.0572791217578672\n",
      "grad\t 2.0 4.0 -0.22865825405740559\n",
      "grad\t 3.0 6.0 -0.5103652230561284\n",
      "progress: 126 w = 1.9721567417199377 loss= 0.006977223284852352\n",
      "grad\t 1.0 2.0 -0.05568651656012458\n",
      "grad\t 2.0 4.0 -0.22230057410801685\n",
      "grad\t 3.0 6.0 -0.4961748814090967\n",
      "progress: 127 w = 1.972930903692015 loss= 0.006594623774378706\n",
      "grad\t 1.0 2.0 -0.05413819261596986\n",
      "grad\t 2.0 4.0 -0.21611966492295132\n",
      "grad\t 3.0 6.0 -0.48237909210802776\n",
      "progress: 128 w = 1.973683540641662 loss= 0.0062330042984315905\n",
      "grad\t 1.0 2.0 -0.05263291871667608\n",
      "grad\t 2.0 4.0 -0.21011061151697064\n",
      "grad\t 3.0 6.0 -0.4689668849058801\n",
      "progress: 129 w = 1.9744152510568014 loss= 0.005891214406378561\n",
      "grad\t 1.0 2.0 -0.05116949788639724\n",
      "grad\t 2.0 4.0 -0.20426863556249764\n",
      "grad\t 3.0 6.0 -0.4559275945754937\n",
      "progress: 130 w = 1.9751266167848258 loss= 0.005568166733120113\n",
      "grad\t 1.0 2.0 -0.049746766430348366\n",
      "grad\t 2.0 4.0 -0.19858909158994997\n",
      "grad\t 3.0 6.0 -0.4432508524287684\n",
      "progress: 131 w = 1.975818203495275 loss= 0.005262833539763436\n",
      "grad\t 1.0 2.0 -0.048363593009450145\n",
      "grad\t 2.0 4.0 -0.19306746329372437\n",
      "grad\t 3.0 6.0 -0.4309265780715954\n",
      "progress: 132 w = 1.9764905611296497 loss= 0.004974243443988601\n",
      "grad\t 1.0 2.0 -0.04701887774070057\n",
      "grad\t 2.0 4.0 -0.18769935994087739\n",
      "grad\t 3.0 6.0 -0.41894497138804\n",
      "progress: 133 w = 1.9771442243387192 loss= 0.004701478329709174\n",
      "grad\t 1.0 2.0 -0.04571155132256166\n",
      "grad\t 2.0 4.0 -0.18248051287966582\n",
      "grad\t 3.0 6.0 -0.40729650474741774\n",
      "progress: 134 w = 1.9777797129076689 loss= 0.004443670426190564\n",
      "grad\t 1.0 2.0 -0.044440574184662296\n",
      "grad\t 2.0 4.0 -0.1774067721451722\n",
      "grad\t 3.0 6.0 -0.39597191542802435\n",
      "progress: 135 w = 1.9783975321694267 loss= 0.004199999547338614\n",
      "grad\t 1.0 2.0 -0.04320493566114658\n",
      "grad\t 2.0 4.0 -0.17247410315929734\n",
      "grad\t 3.0 6.0 -0.3849621982515501\n",
      "progress: 136 w = 1.9789981734064985 loss= 0.003969690482371578\n",
      "grad\t 1.0 2.0 -0.04200365318700294\n",
      "grad\t 2.0 4.0 -0.16767858352251608\n",
      "grad\t 3.0 6.0 -0.3742585984222586\n",
      "progress: 137 w = 1.9795821142416303 loss= 0.0037520105295764873\n",
      "grad\t 1.0 2.0 -0.04083577151673934\n",
      "grad\t 2.0 4.0 -0.16301639989482375\n",
      "grad\t 3.0 6.0 -0.3638526045652437\n",
      "progress: 138 w = 1.9801498190176072 loss= 0.003546267165303756\n",
      "grad\t 1.0 2.0 -0.03970036196478555\n",
      "grad\t 2.0 4.0 -0.15848384496342405\n",
      "grad\t 3.0 6.0 -0.35373594195836056\n",
      "progress: 139 w = 1.9807017391664938 loss= 0.003351805840782368\n",
      "grad\t 1.0 2.0 -0.03859652166701233\n",
      "grad\t 2.0 4.0 -0.15407731449471385\n",
      "grad\t 3.0 6.0 -0.34390056595219853\n",
      "progress: 140 w = 1.9812383135686078 loss= 0.0031680078997489754\n",
      "grad\t 1.0 2.0 -0.0375233728627844\n",
      "grad\t 2.0 4.0 -0.14979330446823447\n",
      "grad\t 3.0 6.0 -0.33433865557309694\n",
      "progress: 141 w = 1.9817599689015122 loss= 0.002994288610264225\n",
      "grad\t 1.0 2.0 -0.03648006219697564\n",
      "grad\t 2.0 4.0 -0.1456284082903263\n",
      "grad\t 3.0 6.0 -0.3250426073040078\n",
      "progress: 142 w = 1.9822671199793036 loss= 0.0028300953044556733\n",
      "grad\t 1.0 2.0 -0.03546576004139279\n",
      "grad\t 2.0 4.0 -0.14157931408523972\n",
      "grad\t 3.0 6.0 -0.316005029038255\n",
      "progress: 143 w = 1.9827601700824686 loss= 0.0026749056202687096\n",
      "grad\t 1.0 2.0 -0.03447965983506274\n",
      "grad\t 2.0 4.0 -0.13764280206157053\n",
      "grad\t 3.0 6.0 -0.30721873420142565\n",
      "progress: 144 w = 1.9832395112785668 loss= 0.0025282258396316527\n",
      "grad\t 1.0 2.0 -0.03352097744286642\n",
      "grad\t 2.0 4.0 -0.13381574195192236\n",
      "grad\t 3.0 6.0 -0.2986767360366933\n",
      "progress: 145 w = 1.9837055247339983 loss= 0.0023895893177490477\n",
      "grad\t 1.0 2.0 -0.03258895053200339\n",
      "grad\t 2.0 4.0 -0.13009509052375812\n",
      "grad\t 3.0 6.0 -0.29037224204902756\n",
      "progress: 146 w = 1.9841585810171032 loss= 0.0022585549985251942\n",
      "grad\t 1.0 2.0 -0.03168283796579363\n",
      "grad\t 2.0 4.0 -0.1264778891594478\n",
      "grad\t 3.0 6.0 -0.28229864860388787\n",
      "progress: 147 w = 1.9845990403928324 loss= 0.002134706011394468\n",
      "grad\t 1.0 2.0 -0.03080191921433517\n",
      "grad\t 2.0 4.0 -0.12296126150362596\n",
      "grad\t 3.0 6.0 -0.27444953567609076\n",
      "progress: 148 w = 1.9850272531092266 loss= 0.0020176483450964904\n",
      "grad\t 1.0 2.0 -0.029945493781546872\n",
      "grad\t 2.0 4.0 -0.11954241117593511\n",
      "grad\t 3.0 6.0 -0.26681866174468816\n",
      "progress: 149 w = 1.9854435596759288 loss= 0.0019070095941742192\n",
      "grad\t 1.0 2.0 -0.029112880648142436\n",
      "grad\t 2.0 4.0 -0.116218619547384\n",
      "grad\t 3.0 6.0 -0.2593999588297642\n",
      "progress: 150 w = 1.9858482911349542 loss= 0.0018024377742091424\n",
      "grad\t 1.0 2.0 -0.028303417730091596\n",
      "grad\t 2.0 4.0 -0.11298724357852485\n",
      "grad\t 3.0 6.0 -0.2521875276672656\n",
      "progress: 151 w = 1.9862417693239303 loss= 0.0017036002020235357\n",
      "grad\t 1.0 2.0 -0.027516461352139476\n",
      "grad\t 2.0 4.0 -0.10984571371774088\n",
      "grad\t 3.0 6.0 -0.24517563301799683\n",
      "progress: 152 w = 1.986624307132018 loss= 0.0016101824372872998\n",
      "grad\t 1.0 2.0 -0.02675138573596403\n",
      "grad\t 2.0 4.0 -0.10679153185796864\n",
      "grad\t 3.0 6.0 -0.23835869910698548\n",
      "progress: 153 w = 1.986996208748719 loss= 0.0015218872821620567\n",
      "grad\t 1.0 2.0 -0.026007582502562077\n",
      "grad\t 2.0 4.0 -0.10382226935022842\n",
      "grad\t 3.0 6.0 -0.23173130518971163\n",
      "progress: 154 w = 1.9873577699057612 loss= 0.001438433835801132\n",
      "grad\t 1.0 2.0 -0.02528446018847763\n",
      "grad\t 2.0 4.0 -0.10093556507240287\n",
      "grad\t 3.0 6.0 -0.22528818124159855\n",
      "progress: 155 w = 1.9877092781122638 loss= 0.0013595566006950755\n",
      "grad\t 1.0 2.0 -0.024581443775472334\n",
      "grad\t 2.0 4.0 -0.09812912355168635\n",
      "grad\t 3.0 6.0 -0.21902420376736487\n",
      "progress: 156 w = 1.9880510128833584 loss= 0.0012850046380230358\n",
      "grad\t 1.0 2.0 -0.023897974233283215\n",
      "grad\t 2.0 4.0 -0.09540071313926646\n",
      "grad\t 3.0 6.0 -0.21293439172684359\n",
      "progress: 157 w = 1.9883832459624577 loss= 0.0012145407693188192\n",
      "grad\t 1.0 2.0 -0.023233508075084508\n",
      "grad\t 2.0 4.0 -0.09274816423573817\n",
      "grad\t 3.0 6.0 -0.20701390257417174\n",
      "progress: 158 w = 1.9887062415373427 loss= 0.0011479408219155995\n",
      "grad\t 1.0 2.0 -0.022587516925314688\n",
      "grad\t 2.0 4.0 -0.09016936756585636\n",
      "grad\t 3.0 6.0 -0.20125802840699336\n",
      "progress: 159 w = 1.9890202564502408 loss= 0.001084992915766288\n",
      "grad\t 1.0 2.0 -0.021959487099518338\n",
      "grad\t 2.0 4.0 -0.08766227250127656\n",
      "grad\t 3.0 6.0 -0.19566219222285142\n",
      "progress: 160 w = 1.9893255404020644 loss= 0.0010254967893716272\n",
      "grad\t 1.0 2.0 -0.02134891919587112\n",
      "grad\t 2.0 4.0 -0.08522488542991802\n",
      "grad\t 3.0 6.0 -0.1902219442795765\n",
      "progress: 161 w = 1.9896223361509697 loss= 0.000969263162671242\n",
      "grad\t 1.0 2.0 -0.020755327698060544\n",
      "grad\t 2.0 4.0 -0.08285526817065758\n",
      "grad\t 3.0 6.0 -0.1849329585569066\n",
      "progress: 162 w = 1.9899108797053953 loss= 0.0009161131348710196\n",
      "grad\t 1.0 2.0 -0.020178240589209473\n",
      "grad\t 2.0 4.0 -0.08055153643212343\n",
      "grad\t 3.0 6.0 -0.17979102931649749\n",
      "progress: 163 w = 1.9901914005117332 loss= 0.0008658776152910705\n",
      "grad\t 1.0 2.0 -0.01961719897653369\n",
      "grad\t 2.0 4.0 -0.07831185831432208\n",
      "grad\t 3.0 6.0 -0.1747920677575685\n",
      "progress: 164 w = 1.9904641216367815 loss= 0.0008183967854228933\n",
      "grad\t 1.0 2.0 -0.019071756726436906\n",
      "grad\t 2.0 4.0 -0.07613445285193698\n",
      "grad\t 3.0 6.0 -0.16993209876552484\n",
      "progress: 165 w = 1.9907292599451254 loss= 0.0007735195904854827\n",
      "grad\t 1.0 2.0 -0.01854148010974921\n",
      "grad\t 2.0 4.0 -0.07401758859811913\n",
      "grad\t 3.0 6.0 -0.1652072577510033\n",
      "progress: 166 w = 1.990987026271584 loss= 0.0007311032588620639\n",
      "grad\t 1.0 2.0 -0.018025947456831837\n",
      "grad\t 2.0 4.0 -0.07195958224767196\n",
      "grad\t 3.0 6.0 -0.16061378757680522\n",
      "progress: 167 w = 1.9912376255888653 loss= 0.0006910128478881618\n",
      "grad\t 1.0 2.0 -0.017524748822269398\n",
      "grad\t 2.0 4.0 -0.06995879729849896\n",
      "grad\t 3.0 6.0 -0.15614803557024892\n",
      "progress: 168 w = 1.9914812571705565 loss= 0.0006531208145477499\n",
      "grad\t 1.0 2.0 -0.017037485658887075\n",
      "grad\t 2.0 4.0 -0.06801364275027666\n",
      "grad\t 3.0 6.0 -0.15180645061861853\n",
      "progress: 169 w = 1.9917181147495842 loss= 0.0006173066097095216\n",
      "grad\t 1.0 2.0 -0.016563770500831687\n",
      "grad\t 2.0 4.0 -0.0661225718393208\n",
      "grad\t 3.0 6.0 -0.14758558034536406\n",
      "progress: 170 w = 1.9919483866722696 loss= 0.0005834562946135953\n",
      "grad\t 1.0 2.0 -0.016103226655460823\n",
      "grad\t 2.0 4.0 -0.0642840808085996\n",
      "grad\t 3.0 6.0 -0.1434820683647935\n",
      "progress: 171 w = 1.9921722560480983 loss= 0.0005514621783887982\n",
      "grad\t 1.0 2.0 -0.015655487903803333\n",
      "grad\t 2.0 4.0 -0.06249670771198268\n",
      "grad\t 3.0 6.0 -0.13949265161314628\n",
      "progress: 172 w = 1.9923899008953274 loss= 0.0005212224754464463\n",
      "grad\t 1.0 2.0 -0.01522019820934517\n",
      "grad\t 2.0 4.0 -0.06075903125170612\n",
      "grad\t 3.0 6.0 -0.13561415775380503\n",
      "progress: 173 w = 1.9926014942825423 loss= 0.0004926409816612988\n",
      "grad\t 1.0 2.0 -0.014797011434915408\n",
      "grad\t 2.0 4.0 -0.059069669648183165\n",
      "grad\t 3.0 6.0 -0.13184350265474798\n",
      "progress: 174 w = 1.99280720446628 loss= 0.0004656267683091149\n",
      "grad\t 1.0 2.0 -0.014385591067440195\n",
      "grad\t 2.0 4.0 -0.057427279541220955\n",
      "grad\t 3.0 6.0 -0.12817768793600415\n",
      "progress: 175 w = 1.9930071950248247 loss= 0.0004400938927875456\n",
      "grad\t 1.0 2.0 -0.013985609950350586\n",
      "grad\t 2.0 4.0 -0.05583055492179945\n",
      "grad\t 3.0 6.0 -0.12461379858545385\n",
      "progress: 176 w = 1.9932016249882825 loss= 0.00041596112519950903\n",
      "grad\t 1.0 2.0 -0.013596750023435078\n",
      "grad\t 2.0 4.0 -0.05427822609355282\n",
      "grad\t 3.0 6.0 -0.12114900064081091\n",
      "progress: 177 w = 1.9933906489650401 loss= 0.0003931516899299064\n",
      "grad\t 1.0 2.0 -0.013218702069919708\n",
      "grad\t 2.0 4.0 -0.052769058663120205\n",
      "grad\t 3.0 6.0 -0.11778053893608131\n",
      "progress: 178 w = 1.9935744172647092 loss= 0.00037159302139258923\n",
      "grad\t 1.0 2.0 -0.012851165470581627\n",
      "grad\t 2.0 4.0 -0.05130185255856112\n",
      "grad\t 3.0 6.0 -0.1145057349107077\n",
      "progress: 179 w = 1.9937530760176492 loss= 0.0003512165331714209\n",
      "grad\t 1.0 2.0 -0.012493847964701654\n",
      "grad\t 2.0 4.0 -0.049875441075089455\n",
      "grad\t 3.0 6.0 -0.11132198447959851\n",
      "progress: 180 w = 1.9939267672911685 loss= 0.0003319573998205908\n",
      "grad\t 1.0 2.0 -0.012146465417663066\n",
      "grad\t 2.0 4.0 -0.048488689947310704\n",
      "grad\t 3.0 6.0 -0.10822675596240039\n",
      "progress: 181 w = 1.9940956292024958 loss= 0.0003137543506297877\n",
      "grad\t 1.0 2.0 -0.011808741595008332\n",
      "grad\t 2.0 4.0 -0.04714049644727325\n",
      "grad\t 3.0 6.0 -0.1052175880703139\n",
      "progress: 182 w = 1.9942597960286084 loss= 0.0002965494746986072\n",
      "grad\t 1.0 2.0 -0.011480407942783266\n",
      "grad\t 2.0 4.0 -0.04582978850759112\n",
      "grad\t 3.0 6.0 -0.1022920879489444\n",
      "progress: 183 w = 1.9944193983130076 loss= 0.00028028803669977366\n",
      "grad\t 1.0 2.0 -0.01116120337398474\n",
      "grad\t 2.0 4.0 -0.04455552386894723\n",
      "grad\t 3.0 6.0 -0.09944792927549351\n",
      "progress: 184 w = 1.994574562969526 loss= 0.00026491830274475846\n",
      "grad\t 1.0 2.0 -0.01085087406094809\n",
      "grad\t 2.0 4.0 -0.04331668925130394\n",
      "grad\t 3.0 6.0 -0.09668285040891078\n",
      "progress: 185 w = 1.9947254133832473 loss= 0.0002503913757986501\n",
      "grad\t 1.0 2.0 -0.01054917323350546\n",
      "grad\t 2.0 4.0 -0.04211229954815465\n",
      "grad\t 3.0 6.0 -0.09399465259148165\n",
      "progress: 186 w = 1.9948720695086204 loss= 0.0002366610401197794\n",
      "grad\t 1.0 2.0 -0.010255860982759213\n",
      "grad\t 2.0 4.0 -0.04094139704317534\n",
      "grad\t 3.0 6.0 -0.09138119820036472\n",
      "progress: 187 w = 1.9950146479648467 loss= 0.00022368361422965482\n",
      "grad\t 1.0 2.0 -0.009970704070306535\n",
      "grad\t 2.0 4.0 -0.039803050648663074\n",
      "grad\t 3.0 6.0 -0.08884040904781543\n",
      "progress: 188 w = 1.9951532621286137 loss= 0.00021141781194538838\n",
      "grad\t 1.0 2.0 -0.009693475742772684\n",
      "grad\t 2.0 4.0 -0.03869635516514869\n",
      "grad\t 3.0 6.0 -0.08637026472861464\n",
      "progress: 189 w = 1.9952880222242502 loss= 0.00019982461103244925\n",
      "grad\t 1.0 2.0 -0.009423955551499663\n",
      "grad\t 2.0 4.0 -0.037620430561586815\n",
      "grad\t 3.0 6.0 -0.08396880101346049\n",
      "progress: 190 w = 1.9954190354113768 loss= 0.0001888671290599858\n",
      "grad\t 1.0 2.0 -0.009161929177246453\n",
      "grad\t 2.0 4.0 -0.036574421275567204\n",
      "grad\t 3.0 6.0 -0.08163410828706574\n",
      "progress: 191 w = 1.9955464058701167 loss= 0.0001785105060635933\n",
      "grad\t 1.0 2.0 -0.008907188259766663\n",
      "grad\t 2.0 4.0 -0.035557495532987815\n",
      "grad\t 3.0 6.0 -0.0793643300296285\n",
      "progress: 192 w = 1.9956702348839392 loss= 0.0001687217936423154\n",
      "grad\t 1.0 2.0 -0.008659530232121515\n",
      "grad\t 2.0 4.0 -0.03456884468662835\n",
      "grad\t 3.0 6.0 -0.07715766134055713\n",
      "progress: 193 w = 1.9957906209201985 loss= 0.00015946985013722578\n",
      "grad\t 1.0 2.0 -0.008418758159602913\n",
      "grad\t 2.0 4.0 -0.03360768257313396\n",
      "grad\t 3.0 6.0 -0.07501234750323427\n",
      "progress: 194 w = 1.9959076597084346 loss= 0.00015072524155771748\n",
      "grad\t 1.0 2.0 -0.00818468058313071\n",
      "grad\t 2.0 4.0 -0.03267324488785839\n",
      "grad\t 3.0 6.0 -0.07292668258970103\n",
      "progress: 195 w = 1.9960214443164952 loss= 0.00014246014794074065\n",
      "grad\t 1.0 2.0 -0.007957111367009606\n",
      "grad\t 2.0 4.0 -0.031764788577103076\n",
      "grad\t 3.0 6.0 -0.07089900810409411\n",
      "progress: 196 w = 1.9961320652245433 loss= 0.0001346482748446827\n",
      "grad\t 1.0 2.0 -0.007735869550913321\n",
      "grad\t 2.0 4.0 -0.030881591247245765\n",
      "grad\t 3.0 6.0 -0.06892771166385003\n",
      "progress: 197 w = 1.9962396103970055 loss= 0.00012726476969677342\n",
      "grad\t 1.0 2.0 -0.007520779205989037\n",
      "grad\t 2.0 4.0 -0.030022950590307573\n",
      "grad\t 3.0 6.0 -0.06701122571756812\n",
      "progress: 198 w = 1.9963441653525194 loss= 0.0001202861427274894\n",
      "grad\t 1.0 2.0 -0.007311669294961298\n",
      "grad\t 2.0 4.0 -0.029188183825485936\n",
      "grad\t 3.0 6.0 -0.0651480262984876\n",
      "progress: 199 w = 1.9964458132319383 loss= 0.00011369019224038641\n",
      "预测到4的值为： 7.985783252927753\n"
     ]
    }
   ],
   "source": [
    "# 随机梯度下降\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "w = 1.0\n",
    "lr = 0.001\n",
    "\n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "def loss_funtion(xs, ys):\n",
    "    pre_y = forward(xs)\n",
    "    return (pre_y - ys) ** 2\n",
    "\n",
    "def gradient(x, y):\n",
    "    return 2 * x * (x * w - y)\n",
    "\n",
    "print(\"开始预测4的值：\", forward(4))\n",
    "for epoch in range(200):\n",
    "    for x, y in zip(x_data, y_data):\n",
    "        grad = gradient(x, y)\n",
    "        w = w - lr * grad\n",
    "        print(\"grad\\t\", x, y, grad)\n",
    "        loss = loss_funtion(x, y)\n",
    "    \n",
    "    print(\"progress:\", epoch, \"w =\", w, \"loss=\", loss)\n",
    "print(\"预测到4的值为：\", forward(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}